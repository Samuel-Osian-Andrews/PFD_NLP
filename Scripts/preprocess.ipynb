{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing scraped PFD data\n",
    "\n",
    "\n",
    "The **first** stage of this notebook cleans the scraped PFD data by removing the report's template text and correcting spelling mistakes, mainly via the OpenAI API. \n",
    "\n",
    "The **second** stage performs NLP-specific preprocessing tasks, including tokenisation, stop word removal, lemmatization, and Word2Vec embeddings.\n",
    "\n",
    "Running the notebook sequentially will output two files to the `../Data` directory. `cleaned.csv` will output following the first stage, and `tokenised.json` will output following the second.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see the result of our web scraping script. It produces 5 fields:\n",
    " * The **URL** of the PFD report.\n",
    " * The **ID**, scraped from the judiciary.uk website.\n",
    " * The **Date** of the report. Unhelpfully, this is in a mixture of data formats. We could use the OpenAI API to help with this, but I've left it for now.\n",
    " * The **Receiver**, or who the report was addressed to.\n",
    " * The **Content** of the reports's *concerns* section. This is the main field of interest for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Receiver</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0318</td>\n",
       "      <td>Date of report: 13/06/2024</td>\n",
       "      <td>TO: The Chief Executive, Leicestershire Partne...</td>\n",
       "      <td>During the course of the investigation my inqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0311</td>\n",
       "      <td>Date of report: 07/06/2024</td>\n",
       "      <td>TO: 1. NATIONAL AMBULANCE RESILIENCE UNIT (NAR...</td>\n",
       "      <td>Regulation 28 – After Inquest Document Templat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0298</td>\n",
       "      <td>Date of report: 05/05/2024</td>\n",
       "      <td>TO: 1. CEO of Quora, 2. The Rt Hon Lucy Fraser...</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0297</td>\n",
       "      <td>Date of report: 29/05/2024</td>\n",
       "      <td>TO: Secretary of State for Justice, 1</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0296</td>\n",
       "      <td>Date of report: 03/06/2024</td>\n",
       "      <td>TO: (1) , Chief Executive, Birmingham and Soli...</td>\n",
       "      <td>During the inquest, the evidence revealed matt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0112</td>\n",
       "      <td>Date of report: 20 March 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0106</td>\n",
       "      <td>Date of report: 17 March 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0087</td>\n",
       "      <td>Date of report: 9 March 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0077</td>\n",
       "      <td>Date of report: 4 March 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0072</td>\n",
       "      <td>Date of report: 27 February 2015</td>\n",
       "      <td>TO: Mr K Bromley-Derry, Chief Executive, Newha...</td>\n",
       "      <td>The MATTERS OF CONCERN are as follows:- *-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL              ID  \\\n",
       "0    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0318   \n",
       "1    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0311   \n",
       "2    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0298   \n",
       "3    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0297   \n",
       "4    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0296   \n",
       "..                                                 ...             ...   \n",
       "565  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0112   \n",
       "566  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0106   \n",
       "567  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0087   \n",
       "568  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0077   \n",
       "569  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0072   \n",
       "\n",
       "                                 Date  \\\n",
       "0          Date of report: 13/06/2024   \n",
       "1          Date of report: 07/06/2024   \n",
       "2          Date of report: 05/05/2024   \n",
       "3          Date of report: 29/05/2024   \n",
       "4          Date of report: 03/06/2024   \n",
       "..                                ...   \n",
       "565     Date of report: 20 March 2015   \n",
       "566     Date of report: 17 March 2015   \n",
       "567      Date of report: 9 March 2015   \n",
       "568      Date of report: 4 March 2015   \n",
       "569  Date of report: 27 February 2015   \n",
       "\n",
       "                                              Receiver  \\\n",
       "0    TO: The Chief Executive, Leicestershire Partne...   \n",
       "1    TO: 1. NATIONAL AMBULANCE RESILIENCE UNIT (NAR...   \n",
       "2    TO: 1. CEO of Quora, 2. The Rt Hon Lucy Fraser...   \n",
       "3                TO: Secretary of State for Justice, 1   \n",
       "4    TO: (1) , Chief Executive, Birmingham and Soli...   \n",
       "..                                                 ...   \n",
       "565                                                NaN   \n",
       "566                                                NaN   \n",
       "567                                                NaN   \n",
       "568                                                NaN   \n",
       "569  TO: Mr K Bromley-Derry, Chief Executive, Newha...   \n",
       "\n",
       "                                               Content  \n",
       "0    During the course of the investigation my inqu...  \n",
       "1    Regulation 28 – After Inquest Document Templat...  \n",
       "2    During the course of the inquest the evidence ...  \n",
       "3    During the course of the inquest the evidence ...  \n",
       "4    During the inquest, the evidence revealed matt...  \n",
       "..                                                 ...  \n",
       "565                                                NaN  \n",
       "566                                                NaN  \n",
       "567                                                NaN  \n",
       "568                                                NaN  \n",
       "569         The MATTERS OF CONCERN are as follows:- *-  \n",
       "\n",
       "[570 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/raw.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all reports were scraped successfully, which we can see by `NaN` values in the `Receiver` and `Content` fields. Inspecting the URLs of failed reports shows that this is because these reports are actually images, which have seemingly been scanned in and uploaded.\n",
    "\n",
    "There are alternative Python packages that allow for the reading of text in images, but we'll leave this for now due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 155 out of 570 reports which were unsuccessfully scraped. This reflects 27.19 % of all reports. \n",
      "This leaves us with 415 reports left to analyse.\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows\n",
    "rows = data.shape[0]\n",
    "\n",
    "# Count number of rows that have N/A values in \"Content\" column\n",
    "na_rows = data['Content'].isna().sum()\n",
    "\n",
    "# Print with no spaces\n",
    "print(f\"There are\", na_rows, \"out of\", rows, \"reports which were unsuccessfully scraped.\",\n",
    "      \"This reflects\", round(na_rows/rows*100, 2), \"% of all reports.\",\n",
    "      \"\\nThis leaves us with\", rows - na_rows, \"reports left to analyse.\")\n",
    "\n",
    "# Drop rows with N/A values in \"Content\" column\n",
    "data = data.dropna(subset = ['Content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing intro text & correcting spelling mistakes\n",
    "\n",
    "Notice in the `Content` field of the scraped reports that all text begins with some intro text. This text differs ever so slightly between reports, but it usually around the lines of:\n",
    "\n",
    "> *During the course of the investigation my inquiries revealed matters giving rise to concern. In my opinion there is a risk that future deaths could occur unless action is taken. In the circumstances it is my statutory duty to report to you. The MATTERS OF CONCERN are as follows:*\n",
    "\n",
    "Because of slight grammatical differences between these intro texts across documents, we can't easily use regular expressions to trim them. Instead, we can call the OpenAI API (GPT 3.5 Turbo) to remove this text for us. While we're here, we can also use the model to correct spelling mistakes.\n",
    "\n",
    "Note that for the below code to run, it requires you to set an environmental variable containing your OpenAI API Key (which I've called `api.env` and placed in the `/Scripts` directory). For security reasons, I've hidden my own API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file containing the OpenAI API Key\n",
    "load_dotenv('api.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define our prompt. We tell GPT to remove introduction text from the report.\n",
    "\n",
    "Our placeholder (`{report_text}`) allows us to dynamically insert each report content into the prompt. We're essentially repeating the prompt by iterating over each report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert in removing leading introductions to reports. You must return the provided report content with the leading introduction taken out. You must abide by the below instructions:\n",
    "1. *Never* change the main content of the report; only ever remove the leading introductory statement.\n",
    "2. Always remove things like 'matters of concern are as follows' and 'my findings are as follows', etc; I am only interested in the core content of the report.\n",
    "3. If you cannot remove the leading introduction (or indeed you cannot find a leading introduction), you must delete the entire report and provide \"NaN\" as your response - nothing else whatsoever.\n",
    "4. You must not, under any circumstances, respond in your own 'voice' (for example, you must not declare that you cannot find a leading introduction, or similar). In these cases, you must simply remove the entire report and provide \"NaN\" - nothing else whatsoever.\n",
    "\n",
    "Here is an example of a leading introductory statement which should be removed from your response:\n",
    "\"During the course of the investigation my inquiries revealed matters giving rise to concern. In my opinion there is a risk that future deaths could occur unless action is taken. In the circumstances it is my statutory duty to report to you. The MATTERS OF CONCERN are as follows:\"\n",
    "\n",
    "Your turn!\n",
    "Report text:\n",
    "{report_text}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function called `build_prompt` which takes a piece of report text and dynamically constructs a prompt for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict # (...for type hints)\n",
    "\n",
    "# Construct prompts for each given report text\n",
    "# ... \"->\" is a type hint; it tells you what type of object the function should return\n",
    "def build_prompt(report_text: str) -> List[Dict[str, str]]:\n",
    "    # OpenAI 'messages' take a list of dictionaries, each with a 'role' and 'content' key. \n",
    "    # Role can be 'system', 'user', or 'assistant' (LLM replies as assistant); content is the text the LLM sees.\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(report_text = report_text)}, # ...applies prompt dynamically with given report content\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply this function to each report by for looping over each element of the 'Contents' field. \n",
    "\n",
    "We also only display the first 600 characters for each report. This is because the LLM doesn't need to see the entirety of each report to trim the intro text. This saves on token processing, making it cheaper and faster to run.\n",
    "\n",
    "*Note the below code chunk will cost approximately $0.11 (£0.09) to run, assuming 415 reports.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning row 0 of 415\n",
      "Cleaning row 1 of 415\n",
      "Cleaning row 2 of 415\n",
      "Cleaning row 3 of 415\n",
      "Cleaning row 4 of 415\n",
      "Cleaning row 5 of 415\n",
      "Cleaning row 6 of 415\n",
      "Cleaning row 7 of 415\n",
      "Cleaning row 8 of 415\n",
      "Cleaning row 9 of 415\n",
      "Cleaning row 10 of 415\n",
      "Cleaning row 11 of 415\n",
      "Cleaning row 12 of 415\n",
      "Cleaning row 13 of 415\n",
      "Cleaning row 14 of 415\n",
      "Cleaning row 15 of 415\n",
      "Cleaning row 16 of 415\n",
      "Cleaning row 17 of 415\n",
      "Cleaning row 18 of 415\n",
      "Cleaning row 19 of 415\n",
      "Cleaning row 20 of 415\n",
      "Cleaning row 21 of 415\n",
      "Cleaning row 22 of 415\n",
      "Cleaning row 23 of 415\n",
      "Cleaning row 24 of 415\n",
      "Cleaning row 25 of 415\n",
      "Cleaning row 26 of 415\n",
      "Cleaning row 27 of 415\n",
      "Cleaning row 28 of 415\n",
      "Cleaning row 29 of 415\n",
      "Cleaning row 30 of 415\n",
      "Cleaning row 31 of 415\n",
      "Cleaning row 32 of 415\n",
      "Cleaning row 33 of 415\n",
      "Cleaning row 34 of 415\n",
      "Cleaning row 35 of 415\n",
      "Cleaning row 36 of 415\n",
      "Cleaning row 37 of 415\n",
      "Cleaning row 38 of 415\n",
      "Cleaning row 39 of 415\n",
      "Cleaning row 40 of 415\n",
      "Cleaning row 41 of 415\n",
      "Cleaning row 42 of 415\n",
      "Cleaning row 43 of 415\n",
      "Cleaning row 44 of 415\n",
      "Cleaning row 45 of 415\n",
      "Cleaning row 46 of 415\n",
      "Cleaning row 47 of 415\n",
      "Cleaning row 48 of 415\n",
      "Cleaning row 49 of 415\n",
      "Cleaning row 50 of 415\n",
      "Cleaning row 51 of 415\n",
      "Cleaning row 52 of 415\n",
      "Cleaning row 53 of 415\n",
      "Cleaning row 54 of 415\n",
      "Cleaning row 55 of 415\n",
      "Cleaning row 56 of 415\n",
      "Cleaning row 57 of 415\n",
      "Cleaning row 58 of 415\n",
      "Cleaning row 59 of 415\n",
      "Cleaning row 60 of 415\n",
      "Cleaning row 61 of 415\n",
      "Cleaning row 62 of 415\n",
      "Cleaning row 63 of 415\n",
      "Cleaning row 64 of 415\n",
      "Cleaning row 65 of 415\n",
      "Cleaning row 66 of 415\n",
      "Cleaning row 67 of 415\n",
      "Cleaning row 68 of 415\n",
      "Cleaning row 69 of 415\n",
      "Cleaning row 70 of 415\n",
      "Cleaning row 71 of 415\n",
      "Cleaning row 72 of 415\n",
      "Cleaning row 73 of 415\n",
      "Cleaning row 74 of 415\n",
      "Cleaning row 75 of 415\n",
      "Cleaning row 76 of 415\n",
      "Cleaning row 77 of 415\n",
      "Cleaning row 78 of 415\n",
      "Cleaning row 79 of 415\n",
      "Cleaning row 80 of 415\n",
      "Cleaning row 81 of 415\n",
      "Cleaning row 82 of 415\n",
      "Cleaning row 83 of 415\n",
      "Cleaning row 84 of 415\n",
      "Cleaning row 85 of 415\n",
      "Cleaning row 86 of 415\n",
      "Cleaning row 87 of 415\n",
      "Cleaning row 88 of 415\n",
      "Cleaning row 89 of 415\n",
      "Cleaning row 90 of 415\n",
      "Cleaning row 91 of 415\n",
      "Cleaning row 92 of 415\n",
      "Cleaning row 93 of 415\n",
      "Cleaning row 94 of 415\n",
      "Cleaning row 95 of 415\n",
      "Cleaning row 96 of 415\n",
      "Cleaning row 97 of 415\n",
      "Cleaning row 98 of 415\n",
      "Cleaning row 99 of 415\n",
      "Cleaning row 100 of 415\n",
      "Cleaning row 101 of 415\n",
      "Cleaning row 102 of 415\n",
      "Cleaning row 103 of 415\n",
      "Cleaning row 104 of 415\n",
      "Cleaning row 105 of 415\n",
      "Cleaning row 106 of 415\n",
      "Cleaning row 107 of 415\n",
      "Cleaning row 108 of 415\n",
      "Cleaning row 109 of 415\n",
      "Cleaning row 110 of 415\n",
      "Cleaning row 111 of 415\n",
      "Cleaning row 112 of 415\n",
      "Cleaning row 113 of 415\n",
      "Cleaning row 114 of 415\n",
      "Cleaning row 115 of 415\n",
      "Cleaning row 116 of 415\n",
      "Cleaning row 117 of 415\n",
      "Cleaning row 118 of 415\n",
      "Cleaning row 119 of 415\n",
      "Cleaning row 120 of 415\n",
      "Cleaning row 121 of 415\n",
      "Cleaning row 122 of 415\n",
      "Cleaning row 123 of 415\n",
      "Cleaning row 124 of 415\n",
      "Cleaning row 125 of 415\n",
      "Cleaning row 126 of 415\n",
      "Cleaning row 127 of 415\n",
      "Cleaning row 128 of 415\n",
      "Cleaning row 129 of 415\n",
      "Cleaning row 130 of 415\n",
      "Cleaning row 131 of 415\n",
      "Cleaning row 132 of 415\n",
      "Cleaning row 133 of 415\n",
      "Cleaning row 134 of 415\n",
      "Cleaning row 135 of 415\n",
      "Cleaning row 136 of 415\n",
      "Cleaning row 137 of 415\n",
      "Cleaning row 138 of 415\n",
      "Cleaning row 139 of 415\n",
      "Cleaning row 140 of 415\n",
      "Cleaning row 141 of 415\n",
      "Cleaning row 142 of 415\n",
      "Cleaning row 143 of 415\n",
      "Cleaning row 144 of 415\n",
      "Cleaning row 145 of 415\n",
      "Cleaning row 146 of 415\n",
      "Cleaning row 147 of 415\n",
      "Cleaning row 148 of 415\n",
      "Cleaning row 149 of 415\n",
      "Cleaning row 150 of 415\n",
      "Cleaning row 151 of 415\n",
      "Cleaning row 152 of 415\n",
      "Cleaning row 153 of 415\n",
      "Cleaning row 154 of 415\n",
      "Cleaning row 155 of 415\n",
      "Cleaning row 156 of 415\n",
      "Cleaning row 157 of 415\n",
      "Cleaning row 158 of 415\n",
      "Cleaning row 159 of 415\n",
      "Cleaning row 160 of 415\n",
      "Cleaning row 161 of 415\n",
      "Cleaning row 162 of 415\n",
      "Cleaning row 163 of 415\n",
      "Cleaning row 164 of 415\n",
      "Cleaning row 165 of 415\n",
      "Cleaning row 166 of 415\n",
      "Cleaning row 167 of 415\n",
      "Cleaning row 168 of 415\n",
      "Cleaning row 169 of 415\n",
      "Cleaning row 170 of 415\n",
      "Cleaning row 171 of 415\n",
      "Cleaning row 172 of 415\n",
      "Cleaning row 173 of 415\n",
      "Cleaning row 174 of 415\n",
      "Cleaning row 175 of 415\n",
      "Cleaning row 176 of 415\n",
      "Cleaning row 177 of 415\n",
      "Cleaning row 178 of 415\n",
      "Cleaning row 179 of 415\n",
      "Cleaning row 180 of 415\n",
      "Cleaning row 181 of 415\n",
      "Cleaning row 182 of 415\n",
      "Cleaning row 183 of 415\n",
      "Cleaning row 184 of 415\n",
      "Cleaning row 185 of 415\n",
      "Cleaning row 186 of 415\n",
      "Cleaning row 187 of 415\n",
      "Cleaning row 188 of 415\n",
      "Cleaning row 189 of 415\n",
      "Cleaning row 190 of 415\n",
      "Cleaning row 191 of 415\n",
      "Cleaning row 192 of 415\n",
      "Cleaning row 193 of 415\n",
      "Cleaning row 194 of 415\n",
      "Cleaning row 195 of 415\n",
      "Cleaning row 196 of 415\n",
      "Cleaning row 197 of 415\n",
      "Cleaning row 198 of 415\n",
      "Cleaning row 199 of 415\n",
      "Cleaning row 200 of 415\n",
      "Cleaning row 201 of 415\n",
      "Cleaning row 202 of 415\n",
      "Cleaning row 203 of 415\n",
      "Cleaning row 204 of 415\n",
      "Cleaning row 205 of 415\n",
      "Cleaning row 206 of 415\n",
      "Cleaning row 207 of 415\n",
      "Cleaning row 208 of 415\n",
      "Cleaning row 209 of 415\n",
      "Cleaning row 210 of 415\n",
      "Cleaning row 211 of 415\n",
      "Cleaning row 212 of 415\n",
      "Cleaning row 213 of 415\n",
      "Cleaning row 214 of 415\n",
      "Cleaning row 215 of 415\n",
      "Cleaning row 216 of 415\n",
      "Cleaning row 217 of 415\n",
      "Cleaning row 218 of 415\n",
      "Cleaning row 219 of 415\n",
      "Cleaning row 220 of 415\n",
      "Cleaning row 221 of 415\n",
      "Cleaning row 222 of 415\n",
      "Cleaning row 223 of 415\n",
      "Cleaning row 224 of 415\n",
      "Cleaning row 225 of 415\n",
      "Cleaning row 226 of 415\n",
      "Cleaning row 227 of 415\n",
      "Cleaning row 228 of 415\n",
      "Cleaning row 229 of 415\n",
      "Cleaning row 230 of 415\n",
      "Cleaning row 231 of 415\n",
      "Cleaning row 232 of 415\n",
      "Cleaning row 233 of 415\n",
      "Cleaning row 234 of 415\n",
      "Cleaning row 235 of 415\n",
      "Cleaning row 236 of 415\n",
      "Cleaning row 237 of 415\n",
      "Cleaning row 238 of 415\n",
      "Cleaning row 239 of 415\n",
      "Cleaning row 240 of 415\n",
      "Cleaning row 241 of 415\n",
      "Cleaning row 242 of 415\n",
      "Cleaning row 243 of 415\n",
      "Cleaning row 244 of 415\n",
      "Cleaning row 245 of 415\n",
      "Cleaning row 246 of 415\n",
      "Cleaning row 247 of 415\n",
      "Cleaning row 248 of 415\n",
      "Cleaning row 249 of 415\n",
      "Cleaning row 250 of 415\n",
      "Cleaning row 251 of 415\n",
      "Cleaning row 252 of 415\n",
      "Cleaning row 253 of 415\n",
      "Cleaning row 254 of 415\n",
      "Cleaning row 255 of 415\n",
      "Cleaning row 256 of 415\n",
      "Cleaning row 257 of 415\n",
      "Cleaning row 258 of 415\n",
      "Cleaning row 259 of 415\n",
      "Cleaning row 260 of 415\n",
      "Cleaning row 261 of 415\n",
      "Cleaning row 262 of 415\n",
      "Cleaning row 263 of 415\n",
      "Cleaning row 264 of 415\n",
      "Cleaning row 265 of 415\n",
      "Cleaning row 266 of 415\n",
      "Cleaning row 267 of 415\n",
      "Cleaning row 268 of 415\n",
      "Cleaning row 269 of 415\n",
      "Cleaning row 270 of 415\n",
      "Cleaning row 271 of 415\n",
      "Cleaning row 272 of 415\n",
      "Cleaning row 273 of 415\n",
      "Cleaning row 274 of 415\n",
      "Cleaning row 275 of 415\n",
      "Cleaning row 276 of 415\n",
      "Cleaning row 277 of 415\n",
      "Cleaning row 278 of 415\n",
      "Cleaning row 279 of 415\n",
      "Cleaning row 280 of 415\n",
      "Cleaning row 281 of 415\n",
      "Cleaning row 282 of 415\n",
      "Cleaning row 283 of 415\n",
      "Cleaning row 284 of 415\n",
      "Cleaning row 285 of 415\n",
      "Cleaning row 286 of 415\n",
      "Cleaning row 287 of 415\n",
      "Cleaning row 288 of 415\n",
      "Cleaning row 289 of 415\n",
      "Cleaning row 290 of 415\n",
      "Cleaning row 291 of 415\n",
      "Cleaning row 292 of 415\n",
      "Cleaning row 293 of 415\n",
      "Cleaning row 294 of 415\n",
      "Cleaning row 295 of 415\n",
      "Cleaning row 296 of 415\n",
      "Cleaning row 297 of 415\n",
      "Cleaning row 298 of 415\n",
      "Cleaning row 299 of 415\n",
      "Cleaning row 300 of 415\n",
      "Cleaning row 301 of 415\n",
      "Cleaning row 302 of 415\n",
      "Cleaning row 303 of 415\n",
      "Cleaning row 304 of 415\n",
      "Cleaning row 305 of 415\n",
      "Cleaning row 306 of 415\n",
      "Cleaning row 307 of 415\n",
      "Cleaning row 308 of 415\n",
      "Cleaning row 309 of 415\n",
      "Cleaning row 310 of 415\n",
      "Cleaning row 311 of 415\n",
      "Cleaning row 312 of 415\n",
      "Cleaning row 313 of 415\n",
      "Cleaning row 314 of 415\n",
      "Cleaning row 315 of 415\n",
      "Cleaning row 316 of 415\n",
      "Cleaning row 317 of 415\n",
      "Cleaning row 318 of 415\n",
      "Cleaning row 319 of 415\n",
      "Cleaning row 320 of 415\n",
      "Cleaning row 321 of 415\n",
      "Cleaning row 322 of 415\n",
      "Cleaning row 323 of 415\n",
      "Cleaning row 324 of 415\n",
      "Cleaning row 325 of 415\n",
      "Cleaning row 326 of 415\n",
      "Cleaning row 327 of 415\n",
      "Cleaning row 328 of 415\n",
      "Cleaning row 329 of 415\n",
      "Cleaning row 330 of 415\n",
      "Cleaning row 331 of 415\n",
      "Cleaning row 332 of 415\n",
      "Cleaning row 333 of 415\n",
      "Cleaning row 334 of 415\n",
      "Cleaning row 335 of 415\n",
      "Cleaning row 336 of 415\n",
      "Cleaning row 337 of 415\n",
      "Cleaning row 338 of 415\n",
      "Cleaning row 339 of 415\n",
      "Cleaning row 340 of 415\n",
      "Cleaning row 341 of 415\n",
      "Cleaning row 342 of 415\n",
      "Cleaning row 343 of 415\n",
      "Cleaning row 344 of 415\n",
      "Cleaning row 345 of 415\n",
      "Cleaning row 346 of 415\n",
      "Cleaning row 347 of 415\n",
      "Cleaning row 348 of 415\n",
      "Cleaning row 349 of 415\n",
      "Cleaning row 350 of 415\n",
      "Cleaning row 351 of 415\n",
      "Cleaning row 352 of 415\n",
      "Cleaning row 353 of 415\n",
      "Cleaning row 354 of 415\n",
      "Cleaning row 355 of 415\n",
      "Cleaning row 356 of 415\n",
      "Cleaning row 357 of 415\n",
      "Cleaning row 358 of 415\n",
      "Cleaning row 359 of 415\n",
      "Cleaning row 360 of 415\n",
      "Cleaning row 361 of 415\n",
      "Cleaning row 362 of 415\n",
      "Cleaning row 363 of 415\n",
      "Cleaning row 364 of 415\n",
      "Cleaning row 365 of 415\n",
      "Cleaning row 366 of 415\n",
      "Cleaning row 367 of 415\n",
      "Cleaning row 368 of 415\n",
      "Cleaning row 369 of 415\n",
      "Cleaning row 370 of 415\n",
      "Cleaning row 371 of 415\n",
      "Cleaning row 372 of 415\n",
      "Cleaning row 373 of 415\n",
      "Cleaning row 374 of 415\n",
      "Cleaning row 375 of 415\n",
      "Cleaning row 376 of 415\n",
      "Cleaning row 377 of 415\n",
      "Cleaning row 378 of 415\n",
      "Cleaning row 379 of 415\n",
      "Cleaning row 380 of 415\n",
      "Cleaning row 381 of 415\n",
      "Cleaning row 382 of 415\n",
      "Cleaning row 383 of 415\n",
      "Cleaning row 384 of 415\n",
      "Cleaning row 385 of 415\n",
      "Cleaning row 386 of 415\n",
      "Cleaning row 387 of 415\n",
      "Cleaning row 388 of 415\n",
      "Cleaning row 389 of 415\n",
      "Cleaning row 390 of 415\n",
      "Cleaning row 391 of 415\n",
      "Cleaning row 392 of 415\n",
      "Cleaning row 393 of 415\n",
      "Cleaning row 394 of 415\n",
      "Cleaning row 395 of 415\n",
      "Cleaning row 396 of 415\n",
      "Cleaning row 397 of 415\n",
      "Cleaning row 398 of 415\n",
      "Cleaning row 399 of 415\n",
      "Cleaning row 400 of 415\n",
      "Cleaning row 401 of 415\n",
      "Cleaning row 402 of 415\n",
      "Cleaning row 403 of 415\n",
      "Cleaning row 404 of 415\n",
      "Cleaning row 405 of 415\n",
      "Cleaning row 406 of 415\n",
      "Cleaning row 407 of 415\n",
      "Cleaning row 408 of 415\n",
      "Cleaning row 409 of 415\n",
      "Cleaning row 410 of 415\n",
      "Cleaning row 411 of 415\n",
      "Cleaning row 412 of 415\n",
      "Cleaning row 413 of 415\n",
      "Cleaning row 414 of 415\n",
      "Time taken: 8 minutes and 53.12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define empty array\n",
    "new_texts = []\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over each element of \"Content\" field and apply prompt\n",
    "for idx, text in enumerate(data['Content']):\n",
    "    print('Cleaning row {i} of {n}'.format(i=idx, n=len(data)))\n",
    "    text_left = text[0:600] # ...shorten text to first 600 characters\n",
    "    text_right = text[600:] # ...hide the rest of the text from the LLM\n",
    "    try:\n",
    "        result = client.chat.completions.create(messages=build_prompt(report_text=text_left), \n",
    "                                                model=\"gpt-3.5-turbo\", # ...can also use more advanced \"gpt-4-turbo\" or \"gpt-4o\"\n",
    "                                                max_tokens=4096,\n",
    "                                                temperature=None, # ...remove randomness from completions\n",
    "                                                seed=18062024).choices[0].message.content\n",
    "        new_texts.append(result + text_right)\n",
    "        new_text = result + text_right\n",
    "    except:\n",
    "        new_texts.append('ERROR')\n",
    "        print(f'OpenAI error on row {idx}')\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is append our new trimmed texts to our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_611941/2266472108.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['CleanContent'] = new_texts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Receiver</th>\n",
       "      <th>Content</th>\n",
       "      <th>CleanContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0318</td>\n",
       "      <td>Date of report: 13/06/2024</td>\n",
       "      <td>TO: The Chief Executive, Leicestershire Partne...</td>\n",
       "      <td>During the course of the investigation my inqu...</td>\n",
       "      <td>Regulation 28 – After Inquest Document Templat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0311</td>\n",
       "      <td>Date of report: 07/06/2024</td>\n",
       "      <td>TO: 1. NATIONAL AMBULANCE RESILIENCE UNIT (NAR...</td>\n",
       "      <td>Regulation 28 – After Inquest Document Templat...</td>\n",
       "      <td>Regulation 28 – After Inquest Document Templat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0298</td>\n",
       "      <td>Date of report: 05/05/2024</td>\n",
       "      <td>TO: 1. CEO of Quora, 2. The Rt Hon Lucy Fraser...</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0297</td>\n",
       "      <td>Date of report: 29/05/2024</td>\n",
       "      <td>TO: Secretary of State for Justice, 1</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2024-0296</td>\n",
       "      <td>Date of report: 03/06/2024</td>\n",
       "      <td>TO: (1) , Chief Executive, Birmingham and Soli...</td>\n",
       "      <td>During the inquest, the evidence revealed matt...</td>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2016-0037</td>\n",
       "      <td>Date of report: 5 February 2016</td>\n",
       "      <td>TO: 1. Dean for Education Barts and the London...</td>\n",
       "      <td>During the course of the inquest, the evidence...</td>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0465</td>\n",
       "      <td>Date of report: 24 November 2015</td>\n",
       "      <td>TO: Chief Executive, Lancashire Care NHS Found...</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0173</td>\n",
       "      <td>Date of report: 29 April 2015</td>\n",
       "      <td>TO: 1. Ms Wendy Wallace Chief Executive Camden...</td>\n",
       "      <td>During the course of the inquest, the evidence...</td>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0116</td>\n",
       "      <td>Date of report: 24 March 2015</td>\n",
       "      <td>TO: National Offender Management Service, Cliv...</td>\n",
       "      <td>During the course of the inquest the evidence ...</td>\n",
       "      <td>- NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Ref: 2015-0072</td>\n",
       "      <td>Date of report: 27 February 2015</td>\n",
       "      <td>TO: Mr K Bromley-Derry, Chief Executive, Newha...</td>\n",
       "      <td>The MATTERS OF CONCERN are as follows:- *-</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL              ID  \\\n",
       "0    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0318   \n",
       "1    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0311   \n",
       "2    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0298   \n",
       "3    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0297   \n",
       "4    https://www.judiciary.uk/prevention-of-future-...  Ref: 2024-0296   \n",
       "..                                                 ...             ...   \n",
       "555  https://www.judiciary.uk/prevention-of-future-...  Ref: 2016-0037   \n",
       "559  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0465   \n",
       "562  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0173   \n",
       "564  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0116   \n",
       "569  https://www.judiciary.uk/prevention-of-future-...  Ref: 2015-0072   \n",
       "\n",
       "                                 Date  \\\n",
       "0          Date of report: 13/06/2024   \n",
       "1          Date of report: 07/06/2024   \n",
       "2          Date of report: 05/05/2024   \n",
       "3          Date of report: 29/05/2024   \n",
       "4          Date of report: 03/06/2024   \n",
       "..                                ...   \n",
       "555   Date of report: 5 February 2016   \n",
       "559  Date of report: 24 November 2015   \n",
       "562     Date of report: 29 April 2015   \n",
       "564     Date of report: 24 March 2015   \n",
       "569  Date of report: 27 February 2015   \n",
       "\n",
       "                                              Receiver  \\\n",
       "0    TO: The Chief Executive, Leicestershire Partne...   \n",
       "1    TO: 1. NATIONAL AMBULANCE RESILIENCE UNIT (NAR...   \n",
       "2    TO: 1. CEO of Quora, 2. The Rt Hon Lucy Fraser...   \n",
       "3                TO: Secretary of State for Justice, 1   \n",
       "4    TO: (1) , Chief Executive, Birmingham and Soli...   \n",
       "..                                                 ...   \n",
       "555  TO: 1. Dean for Education Barts and the London...   \n",
       "559  TO: Chief Executive, Lancashire Care NHS Found...   \n",
       "562  TO: 1. Ms Wendy Wallace Chief Executive Camden...   \n",
       "564  TO: National Offender Management Service, Cliv...   \n",
       "569  TO: Mr K Bromley-Derry, Chief Executive, Newha...   \n",
       "\n",
       "                                               Content  \\\n",
       "0    During the course of the investigation my inqu...   \n",
       "1    Regulation 28 – After Inquest Document Templat...   \n",
       "2    During the course of the inquest the evidence ...   \n",
       "3    During the course of the inquest the evidence ...   \n",
       "4    During the inquest, the evidence revealed matt...   \n",
       "..                                                 ...   \n",
       "555  During the course of the inquest, the evidence...   \n",
       "559  During the course of the inquest the evidence ...   \n",
       "562  During the course of the inquest, the evidence...   \n",
       "564  During the course of the inquest the evidence ...   \n",
       "569         The MATTERS OF CONCERN are as follows:- *-   \n",
       "\n",
       "                                          CleanContent  \n",
       "0    Regulation 28 – After Inquest Document Templat...  \n",
       "1    Regulation 28 – After Inquest Document Templat...  \n",
       "2    (1) There are questions and answers on Quora’s...  \n",
       "3    (1) The prison service instruction (PSI) 64/20...  \n",
       "4    My principal concern is that when a high-risk ...  \n",
       "..                                                 ...  \n",
       "555  Barts and the London 1. Whilst it was clear to...  \n",
       "559  1. Piotr Kucharz was a Polish gentleman who co...  \n",
       "562  Camden and Islington Trust 1. It seemed from t...  \n",
       "564  - NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...  \n",
       "569                                                NaN  \n",
       "\n",
       "[415 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanContent'] = new_texts\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:  During the course of the investigation my inquiries revealed matters giving rise to concern. In my opinion there is a risk that future deaths could occur unless action is taken. In the circumstances it is my statutory duty to report to you. The MATTERS OF CONCERN are as follows: (brief summary of matters of concern) The gatekeeping assessment included a mental health state examination, where it was the clinical opinion of the mental health practitioner from the Crisis Resolution Home Treatment Team, that Ms Morris required an inpatient hospital admission to a mental health ward as there was an immediate risk to her safety as she was found to be a high risk of walking in front of a car. Whilst Ms Morris agreed to an informal admission, this was not possible at the time of assessment as there were no beds available nationally within the NHS or privately. As an inpatient admission was not possible, the option was to attend the Accident and Regulation 28 – After Inquest Document Template Updated 30/07/2021 Emergency Department or to remain in the community whilst waiting for an inpatient mental health bed to become available. Ms Morris had been informed that if she attended the Accident and Emergency Department, there could be a wait of three days for an inpatient mental health bed to become available. Ms Morris did not wish to wait in the Accident and Emergency Department for three days. A safety plan was agreed that Ms Morris would stay overnight with a family member, and would remain under the care of the Crisis Resolution Home Treatment Team who would review the following morning. The family felt that it was pushed for Ms Morris to stay overnight with a family member as there was no alternative to keep her safe. During the course of the inquest, I heard that there is national pressure on hospital trusts as there is a national increase in people waiting for inpatient beds. I am therefore concerned that there is a risk of future deaths as it is not possible to access inpatient mental health beds at the time of clinical need.\n",
      "CLEANED DATA:  The gatekeeping assessment included a mental health state examination, where it was the clinical opinion of the mental health practitioner from the Crisis Resolution Home Treatment Team, that Ms Morris required an inpatient hospital admission to a mental health ward as there was an immediate risk to her safety as she was found to be a high risk of walking in front of a car. Whilst Ms Morris agreed to an informal admission, this was not possible at the time of assessment as there were no beds available nationally within the NHS or privately. As an inpatient admission was not possible, the option was to attend the Accident and Regulation 28 – After Inquest Document Template Updated 30/07/2021 Emergency Department or to remain in the community whilst waiting for an inpatient mental health bed to become available. Ms Morris had been informed that if she attended the Accident and Emergency Department, there could be a wait of three days for an inpatient mental health bed to become available. Ms Morris did not wish to wait in the Accident and Emergency Department for three days. A safety plan was agreed that Ms Morris would stay overnight with a family member, and would remain under the care of the Crisis Resolution Home Treatment Team who would review the following morning. The family felt that it was pushed for Ms Morris to stay overnight with a family member as there was no alternative to keep her safe. During the course of the inquest, I heard that there is national pressure on hospital trusts as there is a national increase in people waiting for inpatient beds. I am therefore concerned that there is a risk of future deaths as it is not possible to access inpatient mental health beds at the time of clinical need.\n"
     ]
    }
   ],
   "source": [
    "# Compare one example of original and cleaned content\n",
    "print(f'ORIGINAL DATA: ', data['Content'][9])\n",
    "print(f'CLEANED DATA: ', data['CleanContent'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've now removed the intro text, we can note that there is also PDF template text that has been accidentally captured by our web scraping tool. This takes the following format (with changing dates):\n",
    "\n",
    "> Regulation 28 – After Inquest Document Template Updated 30/07/2021\n",
    "\n",
    "We can remove this using regular expressions via the `re` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_611941/1482262510.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['CleanContent'] = data['CleanContent'].apply(lambda x: pattern.sub('', x))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Provide pattern using regex\n",
    "pattern = re.compile(r'Regulation 28 – After Inquest Document Template Updated \\d{2}/\\d{2}/\\d{4}')\n",
    "\n",
    "# Remove pattern from CleanContent field\n",
    "data['CleanContent'] = data['CleanContent'].apply(lambda x: pattern.sub('', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:  During the course of the investigation my inquiries revealed matters giving rise to concern. In my opinion there is a risk that future deaths could occur unless action is taken. In the circumstances it is my statutory duty to report to you. The MATTERS OF CONCERN are as follows: (brief summary of matters of concern) The gatekeeping assessment included a mental health state examination, where it was the clinical opinion of the mental health practitioner from the Crisis Resolution Home Treatment Team, that Ms Morris required an inpatient hospital admission to a mental health ward as there was an immediate risk to her safety as she was found to be a high risk of walking in front of a car. Whilst Ms Morris agreed to an informal admission, this was not possible at the time of assessment as there were no beds available nationally within the NHS or privately. As an inpatient admission was not possible, the option was to attend the Accident and Regulation 28 – After Inquest Document Template Updated 30/07/2021 Emergency Department or to remain in the community whilst waiting for an inpatient mental health bed to become available. Ms Morris had been informed that if she attended the Accident and Emergency Department, there could be a wait of three days for an inpatient mental health bed to become available. Ms Morris did not wish to wait in the Accident and Emergency Department for three days. A safety plan was agreed that Ms Morris would stay overnight with a family member, and would remain under the care of the Crisis Resolution Home Treatment Team who would review the following morning. The family felt that it was pushed for Ms Morris to stay overnight with a family member as there was no alternative to keep her safe. During the course of the inquest, I heard that there is national pressure on hospital trusts as there is a national increase in people waiting for inpatient beds. I am therefore concerned that there is a risk of future deaths as it is not possible to access inpatient mental health beds at the time of clinical need.\n",
      "CLEANED DATA:  The gatekeeping assessment included a mental health state examination, where it was the clinical opinion of the mental health practitioner from the Crisis Resolution Home Treatment Team, that Ms Morris required an inpatient hospital admission to a mental health ward as there was an immediate risk to her safety as she was found to be a high risk of walking in front of a car. Whilst Ms Morris agreed to an informal admission, this was not possible at the time of assessment as there were no beds available nationally within the NHS or privately. As an inpatient admission was not possible, the option was to attend the Accident and  Emergency Department or to remain in the community whilst waiting for an inpatient mental health bed to become available. Ms Morris had been informed that if she attended the Accident and Emergency Department, there could be a wait of three days for an inpatient mental health bed to become available. Ms Morris did not wish to wait in the Accident and Emergency Department for three days. A safety plan was agreed that Ms Morris would stay overnight with a family member, and would remain under the care of the Crisis Resolution Home Treatment Team who would review the following morning. The family felt that it was pushed for Ms Morris to stay overnight with a family member as there was no alternative to keep her safe. During the course of the inquest, I heard that there is national pressure on hospital trusts as there is a national increase in people waiting for inpatient beds. I am therefore concerned that there is a risk of future deaths as it is not possible to access inpatient mental health beds at the time of clinical need.\n"
     ]
    }
   ],
   "source": [
    "# Compare one example of original and cleaned content... again!\n",
    "# ...This time there shouldn't be any \"Regulation 28\" text\n",
    "print(f'ORIGINAL DATA: ', data['Content'][9])\n",
    "print(f'CLEANED DATA: ', data['CleanContent'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to remove any reports where our LLM was unable to identify any introduction text. This issue appears to occur for a small minority of reports where the PDF format is atypical. This results in our web scraping tool scraping the template text but not the actual report contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports our LLM was unable to clean: 15 - Removing...\n"
     ]
    }
   ],
   "source": [
    "nan_count = (data['CleanContent'] == 'NaN').sum()\n",
    "print(f'Number of reports our LLM was unable to clean: {nan_count}', '- Removing...')\n",
    "\n",
    "# Remove rows with \"NaN\" strings in the \"CleanContent\" column\n",
    "data = data[data['CleanContent'] != 'NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "data.to_csv('../Data/cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary fields\n",
    "data = data[['URL', 'CleanContent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise and remove unnecessary words\n",
    "\n",
    "Here we remove stop words (e.g. \"the\", \"my\"), punctuation, numbers and special characters.\n",
    "\n",
    "We then word and sentence-tokenise our report contents (topic modelling primarily uses word-tokenisation, though we also need to sentence tokenise our reports for our word embeddings model later on).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/tmp/ipykernel_611941/3672854419.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ProcessedWords'] = data['CleanContent'].apply(lambda x: preprocess_words(word_tokenize(x)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedWords</th>\n",
       "      <th>ProcessedSentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[[mr, larsen, year, old, male, history, mental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>During the course of the inquest the evidence...</td>\n",
       "      <td>[course, inquest, evidence, revealed, matters,...</td>\n",
       "      <td>[[course, inquest, evidence, revealed, matters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>[[questions, answers, quora, website, provide,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>[[prison, service, instruction, psi, sets, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[[principal, concern, mental, health, patient,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Brenda Morris was allowed weekend leave on ...</td>\n",
       "      <td>[brenda, morris, allowed, weekend, leave, basi...</td>\n",
       "      <td>[[], [brenda, morris, allowed, weekend, leave,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>[[barts, london], [whilst, clear, evidence, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>[[], [piotr, kucharz, polish, gentleman, comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>[[camden, islington, trust], [seemed, evidence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>- NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>[[strips, cell, doors], [deceased, design, eng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "0    https://www.judiciary.uk/prevention-of-future-...   \n",
       "1    https://www.judiciary.uk/prevention-of-future-...   \n",
       "2    https://www.judiciary.uk/prevention-of-future-...   \n",
       "3    https://www.judiciary.uk/prevention-of-future-...   \n",
       "4    https://www.judiciary.uk/prevention-of-future-...   \n",
       "..                                                 ...   \n",
       "554  https://www.judiciary.uk/prevention-of-future-...   \n",
       "555  https://www.judiciary.uk/prevention-of-future-...   \n",
       "559  https://www.judiciary.uk/prevention-of-future-...   \n",
       "562  https://www.judiciary.uk/prevention-of-future-...   \n",
       "564  https://www.judiciary.uk/prevention-of-future-...   \n",
       "\n",
       "                                          CleanContent  \\\n",
       "0     Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "1     During the course of the inquest the evidence...   \n",
       "2    (1) There are questions and answers on Quora’s...   \n",
       "3    (1) The prison service instruction (PSI) 64/20...   \n",
       "4    My principal concern is that when a high-risk ...   \n",
       "..                                                 ...   \n",
       "554  1. Brenda Morris was allowed weekend leave on ...   \n",
       "555  Barts and the London 1. Whilst it was clear to...   \n",
       "559  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "562  Camden and Islington Trust 1. It seemed from t...   \n",
       "564  - NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...   \n",
       "\n",
       "                                        ProcessedWords  \\\n",
       "0    [mr, larsen, year, old, male, history, mental,...   \n",
       "1    [course, inquest, evidence, revealed, matters,...   \n",
       "2    [questions, answers, quora, website, provide, ...   \n",
       "3    [prison, service, instruction, psi, sets, proc...   \n",
       "4    [principal, concern, mental, health, patient, ...   \n",
       "..                                                 ...   \n",
       "554  [brenda, morris, allowed, weekend, leave, basi...   \n",
       "555  [barts, london, whilst, clear, evidence, heard...   \n",
       "559  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "562  [camden, islington, trust, seemed, evidence, h...   \n",
       "564  [strips, cell, doors, deceased, design, engine...   \n",
       "\n",
       "                                    ProcessedSentences  \n",
       "0    [[mr, larsen, year, old, male, history, mental...  \n",
       "1    [[course, inquest, evidence, revealed, matters...  \n",
       "2    [[questions, answers, quora, website, provide,...  \n",
       "3    [[prison, service, instruction, psi, sets, pro...  \n",
       "4    [[principal, concern, mental, health, patient,...  \n",
       "..                                                 ...  \n",
       "554  [[], [brenda, morris, allowed, weekend, leave,...  \n",
       "555  [[barts, london], [whilst, clear, evidence, he...  \n",
       "559  [[], [piotr, kucharz, polish, gentleman, comme...  \n",
       "562  [[camden, islington, trust], [seemed, evidence...  \n",
       "564  [[strips, cell, doors], [deceased, design, eng...  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function for pre-processing words\n",
    "def preprocess_words(words):\n",
    "    # Remove punctuation, special characters, and numbers, and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Apply word tokenization and pre-processing to the content\n",
    "data['ProcessedWords'] = data['CleanContent'].apply(lambda x: preprocess_words(word_tokenize(x)))\n",
    "\n",
    "# Sentence-tokenize the content\n",
    "data['ProcessedSentences'] = data['CleanContent'].apply(sent_tokenize)\n",
    "\n",
    "# Apply word tokenization and pre-processing to each sentence\n",
    "data['ProcessedSentences'] = data['ProcessedSentences'].apply(lambda sentences: [preprocess_words(word_tokenize(sentence)) for sentence in sentences])\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the data\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form. For example, the words \"running\", \"runs\" and \"ran\" all need to be returned to their base form of \"run\".\n",
    "\n",
    "Lemmatization is generally favourable to 'stemming' because the former returns a semantically meaningful output. For example, stemming would return \"better\" as \"bet\" while lemmatization would more appropriately return it as \"good\".\n",
    "\n",
    "We can also enhance this process via 'part-of-speech' (POS) tagging. POS tagging enhances lemmatization by ensuring that word classes (verbs, adjectives, etc.) do not get lemmatized into the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedWords</th>\n",
       "      <th>ProcessedSentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[[mr, larsen, year, old, male, history, mental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>During the course of the inquest the evidence...</td>\n",
       "      <td>[course, inquest, evidence, reveal, matter, gi...</td>\n",
       "      <td>[[course, inquest, evidence, reveal, matter, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "      <td>[[question, answer, quora, website, provide, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "      <td>[[prison, service, instruction, psi, set, proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[[principal, concern, mental, health, patient,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Brenda Morris was allowed weekend leave on ...</td>\n",
       "      <td>[brenda, morris, allow, weekend, leave, basis,...</td>\n",
       "      <td>[[], [brenda, morris, allow, weekend, leave, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "      <td>[[bart, london], [whilst, clear, evidence, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "      <td>[[], [piotr, kucharz, polish, gentleman, comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "      <td>[[camden, islington, trust], [seem, evidence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>- NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "      <td>[[strip, cell, door], [deceased, design, engin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "0    https://www.judiciary.uk/prevention-of-future-...   \n",
       "1    https://www.judiciary.uk/prevention-of-future-...   \n",
       "2    https://www.judiciary.uk/prevention-of-future-...   \n",
       "3    https://www.judiciary.uk/prevention-of-future-...   \n",
       "4    https://www.judiciary.uk/prevention-of-future-...   \n",
       "..                                                 ...   \n",
       "554  https://www.judiciary.uk/prevention-of-future-...   \n",
       "555  https://www.judiciary.uk/prevention-of-future-...   \n",
       "559  https://www.judiciary.uk/prevention-of-future-...   \n",
       "562  https://www.judiciary.uk/prevention-of-future-...   \n",
       "564  https://www.judiciary.uk/prevention-of-future-...   \n",
       "\n",
       "                                          CleanContent  \\\n",
       "0     Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "1     During the course of the inquest the evidence...   \n",
       "2    (1) There are questions and answers on Quora’s...   \n",
       "3    (1) The prison service instruction (PSI) 64/20...   \n",
       "4    My principal concern is that when a high-risk ...   \n",
       "..                                                 ...   \n",
       "554  1. Brenda Morris was allowed weekend leave on ...   \n",
       "555  Barts and the London 1. Whilst it was clear to...   \n",
       "559  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "562  Camden and Islington Trust 1. It seemed from t...   \n",
       "564  - NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...   \n",
       "\n",
       "                                        ProcessedWords  \\\n",
       "0    [mr, larsen, year, old, male, history, mental,...   \n",
       "1    [course, inquest, evidence, reveal, matter, gi...   \n",
       "2    [question, answer, quora, website, provide, in...   \n",
       "3    [prison, service, instruction, psi, set, proce...   \n",
       "4    [principal, concern, mental, health, patient, ...   \n",
       "..                                                 ...   \n",
       "554  [brenda, morris, allow, weekend, leave, basis,...   \n",
       "555  [bart, london, whilst, clear, evidence, heard,...   \n",
       "559  [piotr, kucharz, polish, gentleman, commence, ...   \n",
       "562  [camden, islington, trust, seem, evidence, hea...   \n",
       "564  [strip, cell, door, decease, design, engineer,...   \n",
       "\n",
       "                                    ProcessedSentences  \n",
       "0    [[mr, larsen, year, old, male, history, mental...  \n",
       "1    [[course, inquest, evidence, reveal, matter, g...  \n",
       "2    [[question, answer, quora, website, provide, i...  \n",
       "3    [[prison, service, instruction, psi, set, proc...  \n",
       "4    [[principal, concern, mental, health, patient,...  \n",
       "..                                                 ...  \n",
       "554  [[], [brenda, morris, allow, weekend, leave, b...  \n",
       "555  [[bart, london], [whilst, clear, evidence, hea...  \n",
       "559  [[], [piotr, kucharz, polish, gentleman, comme...  \n",
       "562  [[camden, islington, trust], [seem, evidence, ...  \n",
       "564  [[strip, cell, door], [deceased, design, engin...  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Map POS tags for lemmatization\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialise the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize(tokens):\n",
    "    try:\n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Lemmatize with POS tags\n",
    "        lemmatized_tokens = []\n",
    "        for token, tag in pos_tags:\n",
    "            wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply word tokenization, pre-processing, and lemmatization to the content\n",
    "data['ProcessedWords'] = data['CleanContent'].apply(lambda x: lemmatize(preprocess_words(word_tokenize(x))))\n",
    "\n",
    "# Sentence-tokenize the content\n",
    "data['ProcessedSentences'] = data['CleanContent'].apply(sent_tokenize)\n",
    "\n",
    "# Apply word tokenization, pre-processing, and lemmatization to each sentence\n",
    "data['ProcessedSentences'] = data['ProcessedSentences'].apply(\n",
    "    lambda sentences: [lemmatize(preprocess_words(word_tokenize(sentence))) for sentence in sentences]\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "It's useful to use word embeddings prior to topic modelling in order to capture semantic similarity between certain words. For example, words like 'medicine', 'drugs' and 'prescription' would all be treated independently if we did not use embeddings, despite them having similar meanings.\n",
    "\n",
    "By using word embeddings, we numerically link words with similar usage contexts and therefore increase the chances of our topic models presenting more coherent topics.\n",
    "\n",
    "Below we use a pre-trained Word2Vec model from Gensim.\n",
    "\n",
    "Additionally, we scan for **out-of-vocabulary (OOV)** words. These are words contained within our PFD data that are *not* also contained within our pre-trained model. Where this occurs, this is mostly due to spelling mistakes, specialised terminology, and acronyms. Embeddings vector must be identical in dimension to our word tokens. As a crude solution to OOV words - which our Word2Vec model cannot numerically represent - we assign these words the average of all scores contained within the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique OOV words: 673\n",
      "Total number of times an OOV word is used: 2143\n",
      "All rows have matching dimensions between ProcessedWords and WordEmbeddings.\n",
      "The OOV words are as follows: {'undy', 'hmpss', 'ehcps', 'risley', 'woolwich', 'ravenswood', 'beingliquidated', 'colour', 'tavistock', 'largin', 'ordinators', 'npcc', 'warrn', 'dapdune', 'islington', 'psii', 'ehcp', 'elmley', 'stort', 'conlexts', 'mdt', 'materialise', 'pontypridd', 'atherleigh', 'grosvenor', 'glausiusz', 'rnna', 'lyalushko', 'nanothetical', 'nante', 'ihcbtt', 'usefu', 'sacu', 'afzal', 'scc', 'marac', 'luton', 'onse', 'hca', 'kenneway', 'swp', 'waterhouse', 'categorise', 'communbation', 'bourton', 'vou', 'cmhrs', 'adaptthe', 'gibbins', 'walczak', 'solihull', 'wynnstay', 'wmp', 'tobias', 'davy', 'mlght', 'spft', 'enfield', 'finalise', 'fulwood', 'rcrp', 'rlb', 'licence', 'guldance', 'ipts', 'heathcotes', 'kernow', 'managementthat', 'systmone', 'injanuary', 'cooney', 'medway', 'freeda', 'teague', 'parkwood', 'icd', 'initaitves', 'harpenden', 'signiflcant', 'lch', 'dhr', 'mdts', 'bampfylde', 'cortel', 'corfax', 'manoeuvre', 'speciality', 'sajid', 'bodycam', 'enquiry', 'lpft', 'neas', 'scepticism', 'prh', 'meghan', 'kieran', 'cnwl', 'lockwood', 'mcmanus', 'lewes', 'chelmsford', 'frimley', 'nadia', 'spoe', 'emphasise', 'millview', 'mercia', 'poole', 'trainina', 'nanh', 'chrismas', 'collative', 'govia', 'noemis', 'guedalla', 'gilva', 'aau', 'aylesbury', 'leicestershire', 'gerasimidis', 'pandemicemic', 'cumbria', 'cowley', 'secambs', 'mclellan', 'dwp', 'herefordshire', 'nsft', 'authorise', 'anisation', 'amph', 'ellson', 'llidiard', 'nanre', 'ligatured', 'bloomsbury', 'nanroll', 'bsmhft', 'whv', 'utilise', 'acknowledgement', 'approash', 'upvoting', 'dvpn', 'mtombeni', 'britton', 'dovegate', 'marf', 'demeanour', 'qtlb', 'psir', 'carenotes', 'knownligature', 'llb', 'pontypool', 'centre', 'nane', 'roner', 'impend', 'arezou', 'larsen', 'diagrammatically', 'rhian', 'aneurin', 'eeasin', 'eeast', 'faisan', 'organisational', 'formalise', 'cambridgeshire', 'templeton', 'functus', 'underdevelop', 'chiet', 'icope', 'pco', 'btp', 'grayshott', 'amhp', 'ghct', 'bsarcs', 'pennine', 'claires', 'anglia', 'meagre', 'whittington', 'wherethere', 'ddas', 'emd', 'randomised', 'langley', 'waverley', 'gloucestershire', 'cambs', 'hbbt', 'whitelaw', 'clinicans', 'nanere', 'robyn', 'behaviours', 'onaoina', 'bede', 'datix', 'exsacerbated', 'hbtt', 'callie', 'counselling', 'bpas', 'cypmh', 'gids', 'woodberry', 'ipp', 'anv', 'neighbour', 'authorisation', 'northgate', 'nhse', 'covid', 'odourless', 'powys', 'epjs', 'gynaecologist', 'forchecking', 'moyce', 'ania', 'haik', 'keylocks', 'iewg', 'observationhatch', 'quora', 'behavioural', 'enableble', 'mcnally', 'wyndham', 'nanosses', 'becklin', 'neighbouring', 'costello', 'mollie', 'perrott', 'evelina', 'dcmh', 'shft', 'rsm', 'discrepanciy', 'crht', 'geraint', 'broadland', 'hpcs', 'unravelling', 'clements', 'kirsty', 'jlmping', 'tostevin', 'chrtt', 'sii', 'eifion', 'wilh', 'rowley', 'nanson', 'criticise', 'nanailable', 'elft', 'professlonal', 'hinton', 'zuclopenthixol', 'usk', 'gwynedd', 'irene', 'nomis', 'dlss', 'ofcom', 'tirgari', 'wiltshire', 'stillnot', 'winslow', 'wedgwood', 'eeas', 'houseblock', 'launceston', 'topsham', 'armoury', 'iapt', 'calibre', 'nps', 'camhs', 'peterborough', 'unusuaf', 'horstead', 'warrington', 'esca', 'pfdr', 'offence', 'clinica', 'morrill', 'icb', 'characterise', 'ihbtt', 'awp', 'enteriing', 'tewvs', 'gynaecology', 'askam', 'reorganise', 'stockport', 'haringey', 'neurodiverity', 'nikolyan', 'certifv', 'immedialely', 'chiefcoronersoffice', 'miriam', 'tameside', 'stepps', 'accademic', 'lfb', 'oacmht', 'manon', 'neighbourhood', 'mustafa', 'guildowns', 'katharine', 'haverigg', 'bch', 'benfro', 'ruthin', 'labour', 'cobain', 'programme', 'reprecipitated', 'mhra', 'brittain', 'condron', 'zarins', 'chelmer', 'hadrian', 'frazer', 'nhct', 'wyre', 'derbyshire', 'hmicfrs', 'humber', 'cbt', 'consequetnly', 'igor', 'andnational', 'cmht', 'nany', 'ssri', 'shirehall', 'minueting', 'browne', 'wealstun', 'universtity', 'complianceon', 'sodexo', 'hhj', 'tidey', 'relerral', 'alsoan', 'jaden', 'walsall', 'amitryptyline', 'fylde', 'cjldt', 'dando', 'cadat', 'candour', 'dickinson', 'portchester', 'azra', 'clds', 'lpt', 'prbpranolol', 'redbridge', 'gmp', 'whitchurch', 'marnie', 'practises', 'nantion', 'lpmhss', 'caremap', 'apologise', 'spoaa', 'merseyside', 'thistley', 'pinderfields', 'darrell', 'illhealth', 'favour', 'minimisation', 'theoption', 'mhs', 'dundhal', 'metcalfe', 'oxleas', 'enquire', 'abuhb', 'gawthorpe', 'ofsted', 'nelft', 'bdd', 'lrfaan', 'wcbc', 'addaction', 'recognise', 'probllem', 'keyworkers', 'potentialligatures', 'elli', 'baynham', 'gmmh', 'longstaff', 'standardise', 'thepolicyholder', 'michale', 'mav', 'cmhts', 'regiriie', 'practictioner', 'rpfd', 'manston', 'vour', 'hcps', 'maudsley', 'waddup', 'neurodivergent', 'mindworks', 'nansponding', 'oxevision', 'forryan', 'kaye', 'cmcus', 'roehampton', 'lsotretinion', 'homerton', 'watkins', 'hcas', 'pembrokeshire', 'nottinghamshire', 'csms', 'chanthirakumar', 'coraddress', 'amhps', 'bardoliwalla', 'thisreport', 'songht', 'suooort', 'northumbria', 'hwhct', 'centralise', 'nanconsultations', 'osg', 'forcumbria', 'spocs', 'cwb', 'mqht', 'hillingdon', 'fno', 'personalise', 'lancashire', 'nanhas', 'hwlffordd', 'bcuhb', 'heallh', 'unrecognised', 'rbc', 'psirf', 'organise', 'eluned', 'camh', 'matthewson', 'unauthorised', 'practise', 'ligaturing', 'systemone', 'inmind', 'scrutinised', 'oranisations', 'lorenzo', 'anaesthetic', 'cpft', 'bedfordshire', 'shere', 'dyfed', 'martineau', 'eput', 'barnaby', 'sabp', 'guildford', 'summarising', 'humberside', 'ofrelapse', 'pentonville', 'cpns', 'nanon', 'iopc', 'hewell', 'convinved', 'hmp', 'tooverride', 'piotr', 'rmn', 'tewv', 'bsarc', 'scrutinise', 'learnt', 'keynes', 'ocd', 'doncaster', 'optimise', 'loughborough', 'siwan', 'accadenic', 'cramlington', 'prioritise', 'oskar', 'oic', 'wwr', 'kirkham', 'wokingham', 'talygarn', 'mpft', 'javid', 'ssab', 'ownlife', 'utilised', 'kucharz', 'standardised', 'keywork', 'tolley', 'specialise', 'buckett', 'belmarsh', 'oasys', 'ashortage', 'tokam', 'tyneside', 'thecriminal', 'cnps', 'clopixol', 'harbour', 'waveney', 'recognised', 'treatmentteam', 'ambubag', 'fnos', 'pfds', 'viswambaran', 'nanly', 'kmpt', 'turbutt', 'thrumble', 'gingell', 'malone', 'cqc', 'pcr', 'jacobson', 'eipt', 'tamiem', 'judgement', 'ppo', 'anaesthetist', 'haroer', 'meaningfuly', 'daisu', 'haverfordwest', 'rationalisation', 'fao', 'bmshft', 'depaul', 'thiscase', 'hywel', 'fulfil', 'xuanze', 'ccgs', 'paediatric', 'wedgewood', 'centralised', 'thatavailable', 'bevan', 'gartree', 'particularise', 'lul', 'counsellor', 'rdos', 'vauxhall', 'utilising', 'pimlott', 'actionless', 'realise', 'neuadd', 'pyketts', 'mitigatethis', 'chipperfield', 'suffiently', 'darlington', 'hmrc', 'quidance', 'neurodiverse', 'mias', 'honnor', 'iow', 'oliv', 'aubyn', 'winchester', 'mcloughlin', 'salford', 'thouahts', 'behaviour', 'gsz', 'stevyn', 'dbt', 'belgravia', 'marchessou', 'duignan', 'gloucester', 'dependant', 'thameslink', 'csm', 'agnès', 'rebreathe', 'unfavourably', 'irj', 'endeavour', 'lgbtqia', 'ledto', 'chenyang', 'organisation', 'caernarfon', 'hmac', 'warwickshire', 'reiver', 'westleigh', 'specilically', 'nanbelieve', 'avallable', 'imb', 'donoghue', 'gdpr', 'ucas', 'computerised', 'crhtt', 'delahaye', 'ctmuhb', 'admisssion', 'findlay', 'fme', 'swann', 'nwas', 'tooether', 'rca', 'eupd', 'cwp', 'jeopardise', 'kahssay', 'faraday', 'patien', 'gwent', 'catharine', 'sbp', 'considerahy', 'minimise', 'boughton', 'ligate', 'rco', 'northumberland', 'carlon', 'bsmht', 'hmpps', 'monitorinq', 'moj', 'rutland'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedWords</th>\n",
       "      <th>ProcessedSentences</th>\n",
       "      <th>WordEmbeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[[mr, larsen, year, old, male, history, mental...</td>\n",
       "      <td>[[-0.20410156, 0.084472656, 0.01550293, 0.0830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>During the course of the inquest the evidence...</td>\n",
       "      <td>[course, inquest, evidence, reveal, matter, gi...</td>\n",
       "      <td>[[course, inquest, evidence, reveal, matter, g...</td>\n",
       "      <td>[[0.030639648, 0.07080078, 0.072265625, 0.1464...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "      <td>[[question, answer, quora, website, provide, i...</td>\n",
       "      <td>[[0.10107422, 0.099121094, -0.037597656, 0.265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "      <td>[[prison, service, instruction, psi, set, proc...</td>\n",
       "      <td>[[-0.03564453, -0.14257812, 0.27734375, -0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[[principal, concern, mental, health, patient,...</td>\n",
       "      <td>[[0.046875, -0.23046875, 0.328125, -0.16308594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Brenda Morris was allowed weekend leave on ...</td>\n",
       "      <td>[brenda, morris, allow, weekend, leave, basis,...</td>\n",
       "      <td>[[], [brenda, morris, allow, weekend, leave, b...</td>\n",
       "      <td>[[0.13769531, -0.05053711, 0.15429688, 0.09423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "      <td>[[bart, london], [whilst, clear, evidence, hea...</td>\n",
       "      <td>[[-0.034179688, -0.09716797, -0.03564453, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "      <td>[[], [piotr, kucharz, polish, gentleman, comme...</td>\n",
       "      <td>[[-0.033860672, 0.02294986, 0.01809456, 0.0595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "      <td>[[camden, islington, trust], [seem, evidence, ...</td>\n",
       "      <td>[[-0.052734375, -0.0030517578, -0.00793457, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>- NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "      <td>[[strip, cell, door], [deceased, design, engin...</td>\n",
       "      <td>[[0.15429688, -0.044677734, 0.036621094, -0.14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "0    https://www.judiciary.uk/prevention-of-future-...   \n",
       "1    https://www.judiciary.uk/prevention-of-future-...   \n",
       "2    https://www.judiciary.uk/prevention-of-future-...   \n",
       "3    https://www.judiciary.uk/prevention-of-future-...   \n",
       "4    https://www.judiciary.uk/prevention-of-future-...   \n",
       "..                                                 ...   \n",
       "554  https://www.judiciary.uk/prevention-of-future-...   \n",
       "555  https://www.judiciary.uk/prevention-of-future-...   \n",
       "559  https://www.judiciary.uk/prevention-of-future-...   \n",
       "562  https://www.judiciary.uk/prevention-of-future-...   \n",
       "564  https://www.judiciary.uk/prevention-of-future-...   \n",
       "\n",
       "                                          CleanContent  \\\n",
       "0     Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "1     During the course of the inquest the evidence...   \n",
       "2    (1) There are questions and answers on Quora’s...   \n",
       "3    (1) The prison service instruction (PSI) 64/20...   \n",
       "4    My principal concern is that when a high-risk ...   \n",
       "..                                                 ...   \n",
       "554  1. Brenda Morris was allowed weekend leave on ...   \n",
       "555  Barts and the London 1. Whilst it was clear to...   \n",
       "559  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "562  Camden and Islington Trust 1. It seemed from t...   \n",
       "564  - NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL D...   \n",
       "\n",
       "                                        ProcessedWords  \\\n",
       "0    [mr, larsen, year, old, male, history, mental,...   \n",
       "1    [course, inquest, evidence, reveal, matter, gi...   \n",
       "2    [question, answer, quora, website, provide, in...   \n",
       "3    [prison, service, instruction, psi, set, proce...   \n",
       "4    [principal, concern, mental, health, patient, ...   \n",
       "..                                                 ...   \n",
       "554  [brenda, morris, allow, weekend, leave, basis,...   \n",
       "555  [bart, london, whilst, clear, evidence, heard,...   \n",
       "559  [piotr, kucharz, polish, gentleman, commence, ...   \n",
       "562  [camden, islington, trust, seem, evidence, hea...   \n",
       "564  [strip, cell, door, decease, design, engineer,...   \n",
       "\n",
       "                                    ProcessedSentences  \\\n",
       "0    [[mr, larsen, year, old, male, history, mental...   \n",
       "1    [[course, inquest, evidence, reveal, matter, g...   \n",
       "2    [[question, answer, quora, website, provide, i...   \n",
       "3    [[prison, service, instruction, psi, set, proc...   \n",
       "4    [[principal, concern, mental, health, patient,...   \n",
       "..                                                 ...   \n",
       "554  [[], [brenda, morris, allow, weekend, leave, b...   \n",
       "555  [[bart, london], [whilst, clear, evidence, hea...   \n",
       "559  [[], [piotr, kucharz, polish, gentleman, comme...   \n",
       "562  [[camden, islington, trust], [seem, evidence, ...   \n",
       "564  [[strip, cell, door], [deceased, design, engin...   \n",
       "\n",
       "                                        WordEmbeddings  \n",
       "0    [[-0.20410156, 0.084472656, 0.01550293, 0.0830...  \n",
       "1    [[0.030639648, 0.07080078, 0.072265625, 0.1464...  \n",
       "2    [[0.10107422, 0.099121094, -0.037597656, 0.265...  \n",
       "3    [[-0.03564453, -0.14257812, 0.27734375, -0.105...  \n",
       "4    [[0.046875, -0.23046875, 0.328125, -0.16308594...  \n",
       "..                                                 ...  \n",
       "554  [[0.13769531, -0.05053711, 0.15429688, 0.09423...  \n",
       "555  [[-0.034179688, -0.09716797, -0.03564453, 0.03...  \n",
       "559  [[-0.033860672, 0.02294986, 0.01809456, 0.0595...  \n",
       "562  [[-0.052734375, -0.0030517578, -0.00793457, 0....  \n",
       "564  [[0.15429688, -0.044677734, 0.036621094, -0.14...  \n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "# ...We use the popular Google News data source\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Function to get the word vectors for tokens, replacing OOV with average vector\n",
    "def embed(tokens, model, oov_words):\n",
    "    valid_tokens = [token for token in tokens if token in model.key_to_index]\n",
    "    oov_tokens = [token for token in tokens if token not in model.key_to_index]\n",
    "    oov_words.update(oov_tokens)\n",
    "    \n",
    "    if valid_tokens:\n",
    "        word_vectors = [model[token] for token in valid_tokens]\n",
    "        avg_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        avg_vector = np.zeros(model.vector_size)\n",
    "    \n",
    "    # Replace OOV tokens with the average vector\n",
    "    embeddings = [model[token] if token in model.key_to_index else avg_vector for token in tokens]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Initialize the WordEmbeddings column\n",
    "data['WordEmbeddings'] = None\n",
    "\n",
    "# Initialize a set to store OOV words\n",
    "oov_words = set()\n",
    "oov_count = 0\n",
    "mismatch_rows = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for i, row in data.iterrows():\n",
    "    embeddings = embed(row['ProcessedWords'], model, oov_words)\n",
    "    data.at[i, 'WordEmbeddings'] = embeddings\n",
    "    \n",
    "    # Check for OOV words count and mismatched dimensions\n",
    "    oov_count += len([token for token in row['ProcessedWords'] if token not in model.key_to_index])\n",
    "    if len(embeddings) != len(row['ProcessedWords']):\n",
    "        mismatch_rows.append(i)\n",
    "\n",
    "# Print the total count of OOV words\n",
    "print(f'Total number of unique OOV words: {len(oov_words)}')\n",
    "print(f'Total number of times an OOV word is used: {oov_count}')\n",
    "\n",
    "# Check that all embeddings vectors are identical in dimension to the ProcessedWords column\n",
    "if mismatch_rows:\n",
    "    print(f'Rows with dimension mismatch: {mismatch_rows}')\n",
    "else:\n",
    "    print('All rows have matching dimensions between ProcessedWords and WordEmbeddings.')\n",
    "\n",
    "print(f'The OOV words are as follows: {oov_words}')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nested structure of the data, we need to save it in json format rather than csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('../Data/tokenised.json', orient='split')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
