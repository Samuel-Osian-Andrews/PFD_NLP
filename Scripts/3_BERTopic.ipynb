{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic\n",
    "\n",
    "BERTopic uses BERT embeddings and clustering algorithms to discover topics. Topics are characterised by dense clusters of semantically similar embeddings, identified through dimensionality reduction and clustering. \n",
    "\n",
    "Rather than a model, BERTopic is a framework that contains a handful of sub-models, each providing a necessary step in topic representation. These are:\n",
    "* **Embeddings.** This stage represents our text data as a numeric vector to capture sematic meaning and context. This is a core advantage of BERTopic compared to traditional methods such as LDA.\n",
    "* **Dimensionality reduction.** We then take the above embeddings vector and compresses its size to aid computational performance.\n",
    "* **Clustering.** We then cluster our reduced dimension embeddings via unsupervised methods. This essentially extracts our topics.\n",
    "* **TF-IDF.** 'Term Frequency - Inverse Document Frequency' is the approach taken to extract key words and phrases to represent our topic representations. The TF-IDF approach favours frequent terms but also terms that are unique across our wider text corpus.\n",
    "\n",
    "In BERTopic's modular design, each 'module' is independent, meaning that the specific algorithmic approach can be changed for any component, and the remaining steps will be compatible. \n",
    "\n",
    "Although not originally supported, v0.13 (January 2023) also allows us to approximate a probabilistic topic distribution for each report via '.approximate_distribution'.\n",
    "\n",
    "First, we need to read in our cleaned data.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read report data\n",
    "data = pd.read_csv('../Data/cleaned.csv')\n",
    "\n",
    "# Extract CleanContent column\n",
    "reports = data['CleanContent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embeddings\n",
    "\n",
    "### Sentence splitter\n",
    "Before embedding our text, it's useful to first split our reports into sentences. BERTopic generally performs poorly on larger documents, as this tends to result in noisy topics. \n",
    "\n",
    "Splitting our reports into sentences means that BERTopic will not represent individual reports with a topic out-of-the-box, but we can do this manually (for example, by aggreagating topics within each report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = [sent_tokenize(report) for report in reports]\n",
    "sentences = [sentence for doc in sentences for sentence in doc]\n",
    "\n",
    "# Remove numbers from each sentence\n",
    "def remove_numbers(sentence):\n",
    "    return re.sub(r'\\d+', '', sentence)\n",
    "\n",
    "sentences_without_numbers = [remove_numbers(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculate embeddings\n",
    "\n",
    "We'll likely be tweaking hyperparameters for our eventual BERTopic model. Doing this would ordinarily mean that BERTopic would have to caluclate the embeddings each time we run the model, which is computationally demanding. By pre-calculating the embeddings just once, we can re-run our eventual model at a much faster speed.\n",
    "\n",
    "Below, we download the two best performing models for clustering tasks based on the Hugging Face MTEB leaderboard. Both models have over 7 billion parameters, and are roughty 10GB each. You can read more about these models [here](https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct) and [here](https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from bertopic import BERTopic\n",
    "from bertopic.backend import OpenAIBackend\n",
    "\n",
    "# Activate OpenAI API Key\n",
    "load_dotenv('api.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Get embeddings\n",
    "embedding_model = OpenAIBackend(client, \"text-embedding-3-large\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics identified: 14\n",
      "\n",
      "Topic Info:\n",
      "     Topic  Count                                           Name  \\\n",
      "0      -1     89                 -1_risk_health_evidence_mental   \n",
      "1       0     60               0_coroner_response_action_report   \n",
      "2       1     60                        1_risk_staff_care_trust   \n",
      "3       2     42                     2_prison_hmp_acct_prisoner   \n",
      "4       3     40         3_health_mental_mental health_services   \n",
      "5       4     20          4_medication_prescribed_patient_moore   \n",
      "6       5     19            5_officers_training_police_evidence   \n",
      "7       6     17                       6_content_website_dan_uk   \n",
      "8       7     17      7_health_mental_appointment_mental health   \n",
      "9       8      9            8_students_student_university_staff   \n",
      "10      9      8  9_railway_thameslink_thameslink railway_govia   \n",
      "11     10      7     10_care_care coordinator_coordinator_trust   \n",
      "12     11      6                    11_elft_police_findlay_risk   \n",
      "13     12      6                       12_armoury_self_evans_mr   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [risk, health, evidence, mental, team, mental ...   \n",
      "1   [coroner, response, action, report, chief, chi...   \n",
      "2   [risk, staff, care, trust, ward, evidence, pat...   \n",
      "3   [prison, hmp, acct, prisoner, prisoners, healt...   \n",
      "4   [health, mental, mental health, services, supp...   \n",
      "5   [medication, prescribed, patient, moore, mr mo...   \n",
      "6   [officers, training, police, evidence, force, ...   \n",
      "7   [content, website, dan, uk, evidence, life, su...   \n",
      "8   [health, mental, appointment, mental health, g...   \n",
      "9   [students, student, university, staff, accadem...   \n",
      "10  [railway, thameslink, thameslink railway, govi...   \n",
      "11  [care, care coordinator, coordinator, trust, c...   \n",
      "12  [elft, police, findlay, risk, ms findlay, poli...   \n",
      "13  [armoury, self, evans, mr, sean, police, harm,...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [ Pre-amble Mr Larsen was a 52 year old male w...  \n",
      "1   [NaNisk on the day she died. (3) I refer you t...  \n",
      "2   [- 3 Essex Partnership NHS Foundation Trust. (...  \n",
      "3   [NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DO...  \n",
      "4   [I OFFICIAL SENSITIVE - Zoe was diagnosed at a...  \n",
      "5   [(1) Whilst each individual pharmacy had in-ho...  \n",
      "6   [The evidence of Detective Inspector , Tactica...  \n",
      "7   [The following matters were raised during the ...  \n",
      "8   [On the evidence various issues were addressed...  \n",
      "9   [(1) Lily resided at student accommodation pro...  \n",
      "10  [(1) Following completed suicides on the railw...  \n",
      "11  [From the evidence I heard, the Care Coordinat...  \n",
      "12  [(1) The is a need for the recognition of the ...  \n",
      "13  [- Following his arrest, and before he was int...  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       min_topic_size=5)\n",
    "\n",
    "# Fit the model to data\n",
    "topics, probabilities = topic_model.fit_transform(reports)\n",
    "\n",
    "# Find unique topics\n",
    "unique_topics = set(topics)\n",
    "num_unique_topics = len(unique_topics)\n",
    "\n",
    "print(f\"Number of unique topics identified: {num_unique_topics}\")\n",
    "print(\"\")\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"Topic Info:\\n\", topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
