{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic\n",
    "\n",
    "BERTopic uses BERT embeddings and clustering algorithms to discover topics. Topics are characterised by dense clusters of semantically similar embeddings, identified through dimensionality reduction and clustering. \n",
    "\n",
    "Rather than a model, BERTopic is a framework that contains a handful of sub-models, each providing a necessary step in topic representation. These are:\n",
    "* **Embeddings.** This stage represents our text data as a numeric vector to capture sematic meaning and context. This is a core advantage of BERTopic compared to traditional methods such as LDA.\n",
    "* **Dimensionality reduction.** We then take the above embeddings vector and compresses its size to aid computational performance.\n",
    "* **Clustering.** We then cluster our reduced dimension embeddings via unsupervised methods. This essentially extracts our topics.\n",
    "* **TF-IDF.** 'Term Frequency - Inverse Document Frequency' is the approach taken to extract key words and phrases to represent our topic representations. The TF-IDF approach favours frequent terms but also terms that are unique across our wider text corpus.\n",
    "\n",
    "In BERTopic's modular design, each 'module' is independent, meaning that the specific algorithmic approach can be changed for any component, and the remaining steps will be compatible. \n",
    "\n",
    "Although not originally supported, v0.13 (January 2023) also allows us to approximate a probabilistic topic distribution for each report via '.approximate_distribution'.\n",
    "\n",
    "First, we need to read in our cleaned data.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read report data\n",
    "data = pd.read_csv('../Data/cleaned.csv')\n",
    "\n",
    "# Extract CleanContent column\n",
    "reports = data['CleanContent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embeddings\n",
    "\n",
    "### Sentence splitter\n",
    "Before embedding our text, it's useful to first split our reports into sentences. BERTopic generally performs poorly on larger documents, as this tends to result in noisy topics. \n",
    "\n",
    "Splitting our reports into sentences means that BERTopic will not represent individual reports with a topic out-of-the-box, but we can do this manually (for example, by aggreagating topics within each report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr Larsen reported going through a very diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr Larsen advised the GP that he had placed a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr Larsen’s GP referred him to the CRISIS Home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Larsen was seen regularly by the team and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>It makes no mention of s.136 detentions which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>SODEXO - ITEMS USED TO FACILITATE SUICIDE 12.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>Some prisoners at HMP Peterborough are allowed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>13.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>In addition, the deceased referred to experime...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences\n",
       "0      Pre-amble Mr Larsen was a 52 year old male wi...\n",
       "1     Mr Larsen reported going through a very diffic...\n",
       "2     Mr Larsen advised the GP that he had placed a ...\n",
       "3     Mr Larsen’s GP referred him to the CRISIS Home...\n",
       "4     Mr Larsen was seen regularly by the team and c...\n",
       "...                                                 ...\n",
       "5137  It makes no mention of s.136 detentions which ...\n",
       "5138      SODEXO - ITEMS USED TO FACILITATE SUICIDE 12.\n",
       "5139  Some prisoners at HMP Peterborough are allowed...\n",
       "5140                                                13.\n",
       "5141  In addition, the deceased referred to experime...\n",
       "\n",
       "[5142 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = [sent_tokenize(report) for report in reports]\n",
    "sentences = [sentence for doc in sentences for sentence in doc]\n",
    "sentences = pd.DataFrame(sentences, columns=['sentences'])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Activate OpenAI API Key\n",
    "load_dotenv('api.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"You will be provided with a sentence. You must return the sentence - and nothing else whatsoever- with the following modifications:\n",
    "1. Correct any spelling errors.\n",
    "2. Remove any numbers at the start or end of sentences (e.g. \"There were 2 people\" would be fine).\n",
    "3. Ensure that the sentence is grammatically correct.\n",
    "4. Make the sentence more concise, but do not remove any important information or change the underlying meaning of the sentence.\n",
    "5. Preserve acronyms and abbreviations as they are.\n",
    "6. *Never* respond in your own words; always return the original sentence with the requested modifications only.\n",
    "7. If you cannot find a sentence, simply return what I've given you with no modifications.\n",
    "8. Remove any reference to dates.\n",
    "9. Remove reference to names (e.g. \"Sam went to the store\" should be changed to \"They went to the store\").\n",
    "\n",
    "Your turn! Here is your sentence:\n",
    "{sentence}\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "# Construct prompts for each given report sentence\n",
    "def build_prompt(sentence: str) -> List[Dict[str, str]]:\n",
    "    # OpenAI 'messages' take a list of dictionaries, each with a 'role' and 'content' key. \n",
    "    # Role can be 'system', 'user', or 'assistant' (LLM replies as assistant); content is the text the LLM sees.\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(sentence = sentence)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 4003\n",
      "Original: (3) Not all relevant information was shared between the Child & Adult Mental Health Team about the circumstances disclosed of events on the night of 29th September of Ellis’s failed attempt at hanging as part of a risk assessment.\n",
      "New: Not all relevant information was shared between the Child & Adult Mental Health Team about the circumstances disclosed of events on the night of Ellis’s failed attempt at hanging as part of a risk assessment.\n",
      "\n",
      "\n",
      "Processing sentence 568\n",
      "Original: 4.\n",
      "New: 4.\n",
      "\n",
      "\n",
      "Processing sentence 2003\n",
      "Original: The Trust’s own investigation into events leading to Mr Howe’s death did not consider the full extent of his contacts with mental health services, lacked any meaningful degree of critical analysis of events, and omitted to seek to explore fundamental issues such as access to services from the patient’s perspective.\n",
      "New: The Trust’s own investigation into events leading to Mr Howe’s death did not consider the full extent of their contacts with mental health services, lacked any meaningful degree of critical analysis of events, and omitted to seek to explore fundamental issues such as access to services from the patient’s perspective.\n",
      "\n",
      "\n",
      "Processing sentence 1398\n",
      "Original: Are there any checks in place to avoid duplicity of prescriptions between hospital and GP?\n",
      "New: Are there any checks in place to avoid duplicity of prescriptions between hospital and GP?\n",
      "\n",
      "\n",
      "Processing sentence 4528\n",
      "Original: 4.\n",
      "New: 4.\n",
      "\n",
      "\n",
      "Processing sentence 4329\n",
      "Original: He had started researching schizophrenia on his phone just before his death.\n",
      "New: They had started researching schizophrenia on their phone just before their death.\n",
      "\n",
      "\n",
      "Processing sentence 3215\n",
      "Original: The Inquest heard that care co-ordinators are fundamental to the safe provision of care for high-risk service users.\n",
      "New: The inquest heard that care coordinators are fundamental to the safe provision of care for high-risk service users.\n",
      "\n",
      "\n",
      "Processing sentence 1435\n",
      "Original: 2.\n",
      "New: 2.\n",
      "\n",
      "\n",
      "Processing sentence 2650\n",
      "Original: The treatment Daniel required was twofold.\n",
      "New: The treatment they required was twofold.\n",
      "\n",
      "\n",
      "Processing sentence 4215\n",
      "Original: The National Probation Service Approved Premises Manual 2014 requires Approved Premises to put in place a local procedure for the collection/delivery of residents’ prescribed medication.\n",
      "New: The National Probation Service Approved Premises Manual requires Approved Premises to put in place a local procedure for the collection/delivery of residents’ prescribed medication.\n",
      "\n",
      "\n",
      "Processing sentence 2334\n",
      "Original: He demonstrated a lack of clarity on the point that I found very concerning.\n",
      "New: They demonstrated a lack of clarity on the point that I found very concerning.\n",
      "\n",
      "\n",
      "Processing sentence 991\n",
      "Original: The Prison Staff caring for Wyndham were not aware of the location of known ligature points within the cell.\n",
      "New: The prison staff caring for Wyndham were not aware of the location of known ligature points within the cell.\n",
      "\n",
      "\n",
      "Processing sentence 246\n",
      "Original: On the evidence various issues were addressed and set out in the coroner’s determination and findings and can be referred to for wider reading.\n",
      "New: The evidence addressed various issues in the coroner’s determination and findings for wider reading.\n",
      "\n",
      "\n",
      "Processing sentence 2342\n",
      "Original: Sean, discovering his access to the override key was blocked asked one of the officers to second him into the armoury.\n",
      "New: Discovering their access to the override key was blocked, they asked an officer to accompany them into the armory.\n",
      "\n",
      "\n",
      "Processing sentence 3659\n",
      "Original: - The sharing of examination results and how examinations are marked is complex, confusing and at times capable of appearing misleading.\n",
      "New: The sharing of examination results and how examinations are marked is complex, confusing, and at times capable of appearing misleading.\n",
      "\n",
      "\n",
      "Processing sentence 193\n",
      "Original: She had been writing to Frazer at HMP Winchester through the email a prisoner service.\n",
      "New: They had been writing to Frazer at HMP Winchester through the Email a Prisoner service.\n",
      "\n",
      "\n",
      "Processing sentence 614\n",
      "Original: Surrey County Council 10 Signed: ANNA CRAWFORD Anna Crawford H.M Assistant Coroner for Surrey Dated this 20th day of March 2024\n",
      "New: Surrey County Council. Signed: ANNA CRAWFORD. Anna Crawford. H.M Assistant Coroner for Surrey. Dated this day of March.\n",
      "\n",
      "\n",
      "Processing sentence 955\n",
      "Original: The additional evidence on PFD matters provided by Frimley Health NHS Foundation Trust does not refer to or address the NICE guidelines on self-harm or explain what would now be done differently were a patient such as Mr. EVANS were to be seen again.\n",
      "New: The additional evidence on PFD matters provided by Frimley Health NHS Foundation Trust does not refer to or address the NICE guidelines on self-harm or explain what would now be done differently were a patient such as Mr. EVANS to be seen again.\n",
      "\n",
      "\n",
      "Processing sentence 4226\n",
      "Original: 8 YOUR RESPONSE You are under a duty to respond to this report within 56 days of its date; I may extend that period on request.\n",
      "New: You are under a duty to respond to this report within days of its date; I may extend that period on request.\n",
      "\n",
      "\n",
      "Processing sentence 4955\n",
      "Original: She said that, at the time she started chest compressions, she did not know whether Mr Kahssay was breathing or not breathing.\n",
      "New: She said that, at the time she started chest compressions, she did not know whether Mr. Kahssay was breathing or not.\n",
      "\n",
      "\n",
      "Time taken: 0 minutes and 14.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Define empty array for new texts\n",
    "new_sentences = []\n",
    "original_sentences = []\n",
    "\n",
    "# Sample 20 sentences from the dataframe\n",
    "random.seed(54321)\n",
    "sample_sentences = random.sample(range(len(sentences)), 20)\n",
    "\n",
    "# Start the clock\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each sentence with GPT-3.5 Turbo\n",
    "for count, idx in enumerate(sample_sentences, start=1):\n",
    "    sentence = sentences['sentences'].iloc[idx]\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=build_prompt(sentence),\n",
    "                temperature=0,\n",
    "                seed=18062024\n",
    "            ).choices[0].message.content\n",
    "            \n",
    "            new_sentences.append(response)\n",
    "            \n",
    "            # Print progress and results\n",
    "            print(f\"Processing sentence {idx}\")\n",
    "            print(f\"Original: {sentence}\")\n",
    "            print(f\"New: {response}\\n\")\n",
    "            print(\"\")\n",
    "            \n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {idx}: {e}\")\n",
    "            break\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Define empty array for new texts\n",
    "new_sentences = []\n",
    "original_sentences = []\n",
    "\n",
    "# Sample 20 sentences\n",
    "random.seed(18062024)\n",
    "sampled_indices = random.sample(range(len(sentences)), 20)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each sentence with GPT-3.5 Turbo\n",
    "for idx, sentence in enumerate(sample_sentences):\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=build_prompt(sentence = sentence),\n",
    "                temperature=None,\n",
    "                seed=18062024\n",
    "            ).choices[0].message.content\n",
    "            \n",
    "            new_sentences.append(response)\n",
    "            original_sentences.append(sentence)\n",
    "            \n",
    "            # Print progress and results\n",
    "            print(f\"Processing sentence {idx + 1}/{len(sample_sentences)}\")\n",
    "            print(f\"Original: {sentence}\")\n",
    "            print(f\"New: {new_sentences}\\n\")\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {idx + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')\n",
    "\n",
    "# Create data frame to store \"new_texts\" and \"original_texts\"\n",
    "result_df = pd.DataFrame({'Original Text': original_texts, 'Cleaned Text': new_texts})\n",
    "\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculate embeddings\n",
    "\n",
    "We'll likely be tweaking hyperparameters for our eventual BERTopic model. Doing this would ordinarily mean that BERTopic would have to caluclate the embeddings each time we run the model, which is computationally demanding. By pre-calculating the embeddings just once, we can re-run our eventual model at a much faster speed.\n",
    "\n",
    "Below, we download the two best performing models for clustering tasks based on the Hugging Face MTEB leaderboard. Both models have over 7 billion parameters, and are roughty 10GB each. You can read more about these models [here](https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct) and [here](https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.backend import OpenAIBackend\n",
    "\n",
    "\n",
    "\n",
    "# Get embeddings\n",
    "embedding_model = OpenAIBackend(client, \"text-embedding-3-large\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,3), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       min_topic_size=4)\n",
    "\n",
    "# Fit the model to data\n",
    "topics, probabilities = topic_model.fit_transform(reports)\n",
    "\n",
    "# Find unique topics\n",
    "unique_topics = set(topics)\n",
    "num_unique_topics = len(unique_topics)\n",
    "\n",
    "print(f\"Number of unique topics identified: {num_unique_topics}\")\n",
    "print(\"\")\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"Topic Info:\\n\", topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
