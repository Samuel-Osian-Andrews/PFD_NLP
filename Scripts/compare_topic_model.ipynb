{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing topic modelling techniques\n",
    "\n",
    "There are different topic modelling approaches, each with a different set of advantages and disadvantages. The 'best' modelling technique is far from absolute, and largely depends on the nuances of the text data being analysed. To our knowledge, PFD data has not yet been analysed via NLP or topic modelling techniques, meaning that there exists no literature on the optimal approach(es).\n",
    "\n",
    "This notebook will compare the suitability of 5 topic modelling techniques for PFD 'concerns' data.<br><br><br>\n",
    "\n",
    "\n",
    "1. **Latent Dirichlet Allocation (LDA)**\n",
    "\n",
    "LDA is perhaps the most popular topic modelling technique. It is a probabilistic method that assumes each document is a mixture of various topics (likely suitable for PFD reports which frequently contain multiple concerns). It characterises topics as a 'mixture of words'; the model generates a topic distribution for each document and a word distribution for each topic.\n",
    "\n",
    "LDA *does* require that we pre-define our number of topics.\n",
    "\n",
    "It uses Dirichlet distribution priors to model the distribution of topics in documents and words in topics, providing a more statistically aligned framework for topic modelling.<br><br><br>\n",
    "\n",
    "\n",
    "2. **Correlated Topic Modelling (CTM)**\n",
    "\n",
    "CTM is an extension of LDA that allows for correlations between topics. While it carries over core disadvantages of LDA in terms of less interpretable keyword lists for each topic, its unique contribution is its inclusion of a covariance structure to model topic correlations. This is particularly interesting for our PFD data, where many reports are built from multiple concerns and therefore topics. \n",
    "\n",
    "CTM *does* require us to pre-define our number of topics.<br><br><br>\n",
    "\n",
    "\n",
    "3. **Non-negative Matrix Factorisation (NMF)**\n",
    "\n",
    "NMF is a matrix factorisation technique that decomposes the document-term matrix into two lower-dimensional matrices. Topics are characterised by non-negative components in the factorised matrices, representing the importance of words in topics and topics in documents. Similarly to LDA, it assumes that documents contain multiple topics.\n",
    "\n",
    "NMF *does* require that we pre-define our number of topics.\n",
    "\n",
    "NMF enforces non-negativity constraints. Many report that resulting topic keywords are therefore more interpretable than LDA, with less 'noise' in the keyword lists.<br><br><br>\n",
    "\n",
    "\n",
    "4. **Top2Vec**\n",
    "\n",
    "Topics in Top2Vec are characterised by dense clusters of document and word embeddings. These clusters are identified in a joint embedding space, where both documents and words are represented. It does allow for multiple topics per document; this is achieved through the proximity of document embeddings to multiple topic vectors in the semantic space.\n",
    "\n",
    "Top2Vec does *not* require us to pre-define our number of topics.\n",
    "\n",
    "Top2Vec uses deep learning-based embeddings (e.g., Doc2Vec, Universal Sentence Encoder) to capture the semantic relationships in the text. This method ensures that topics are discovered based on the natural clustering of similar documents and words, leading to a more intuitive and data-driven identification of topics.<br><br><br>\n",
    "\n",
    "\n",
    "5. **BERTopic**\n",
    "\n",
    "BERTopic uses BERT embeddings and clustering algorithms to discover topics. Topics are characterised by dense clusters of semantically similar embeddings, identified through dimensionality reduction and clustering. Although not originally supported, v0.13 (January 2023) allows us to approximate a probabilistic topic distribution for each report via '.approximate_distribution'.\n",
    "\n",
    "BERTopic does *not* require us to pre-define our number of topics.<br><br>\n",
    "\n",
    "\n",
    "### Before we get started\n",
    "\n",
    "There are a few processing steps we need to complete before running this comparison. We will remove stop words, punctuation and numbers from our report content, lemmatize the text, and perform word embeddings.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent\n",
       "ID                                                               \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...\n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...\n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...\n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...\n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...\n",
       "...                                                           ...\n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...\n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...\n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...\n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...\n",
       "Ref: 2015-0072                                                 \"\"\n",
       "\n",
       "[415 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import cleaned data\n",
    "data = pd.read_csv('../Data/cleaned.csv', index_col='ID')\n",
    "\n",
    "# Just keep \"CleanContent\" field\n",
    "data = data[['CleanContent']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuation \n",
    "\n",
    "This stage is vital for removing words like \"the\" and \"my\" from the reports. These words provide unnecessary 'noise' in topic modelling which can result in much less coherent topics.\n",
    "\n",
    "Below, we tokenise the report content and remove stop words, numbers and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...  \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...  \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...  \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...  \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...  \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...  \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...  \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...  \n",
       "Ref: 2015-0072                                                 []  \n",
       "\n",
       "[415 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the report content\n",
    "data['ProcessedContent'] = data['CleanContent'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation, special characters, and numbers, and convert to lowercase\n",
    "data['ProcessedContent'] = data['ProcessedContent'].apply(lambda x: [word.lower() for word in x if word.isalpha()])\n",
    "\n",
    "# Remove stopwords\n",
    "data['ProcessedContent'] = data['ProcessedContent'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the data\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form. For example, the words \"running\", \"runs\" and \"ran\" all need to be returned to their base form of \"run\".\n",
    "\n",
    "Lemmatization is generally favourable to 'stemming' because the former returns a semantically meaningful output. For example, stemming would return \"better\" as \"bet\" while lemmatization would return it as \"good\".\n",
    "\n",
    "We can also enhance this process via 'part-of-speech' (POS) tagging. POS tagging enhances lemmatization by identifying the grammatical role of words. For example, without POS tagging, the word \"lead\" in the sentences \"The lead levels were high\" and \"He will lead the investigation\" could be incorrectly lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map POS tags for lemmatization\n",
    "# ...J = Adjective, R = Adverb, V = Verb, N = Noun\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>[process, triaging, prioritise, ambulance, att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...   \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...   \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                LemmatizedContent  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...  \n",
       "Ref: 2024-0311  [process, triaging, prioritise, ambulance, att...  \n",
       "Ref: 2024-0298  [question, answer, quora, website, provide, in...  \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, set, proce...  \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [bart, london, whilst, clear, evidence, heard,...  \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commence, ...  \n",
       "Ref: 2015-0173  [camden, islington, trust, seem, evidence, hea...  \n",
       "Ref: 2015-0116  [strip, cell, door, decease, design, engineer,...  \n",
       "Ref: 2015-0072                                                 []  \n",
       "\n",
       "[415 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Initialise the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to process tokens\n",
    "def process_content(tokens):\n",
    "    try:\n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Lemmatize with POS tags\n",
    "        lemmatized_tokens = []\n",
    "        for token, tag in pos_tags:\n",
    "            wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the process_content function\n",
    "data['LemmatizedContent'] = data['ProcessedContent'].apply(process_content)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "It's useful to use word embeddings prior to topic modelling in order to capture semantic similarity between certain words. For example, words like 'medicine', 'drugs' and 'prescription' would all be treated independently if we did not use embeddings, despite them potentially having similar meanings.\n",
    "\n",
    "By using word embeddings, we therefore increase the chance of our topic modelling approaches identifying coherent topics within the reports.\n",
    "\n",
    "We can either use word embedding tools (such as Word2Vec) which essentially builds a bespoke embedding framework for our report content, or we can use a pre-trained model courtesy of OpenAI - which is easier to implement but *potentially* less suited to highly domain-specific texts such as PFD reports.\n",
    "\n",
    "For now, we'll use OpenAI's new and more advanced word embedding model (`text-embedding-3-large`), which was released in January 2024.\n",
    "\n",
    "For this to work, we first need to untokenize our data in the `LemmatizedContent` column as OpenAI expects non-tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports exceeding the maximum number of tokens: 0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import python_utils\n",
    "\n",
    "# First, we need to make sure that no report exceeds the max number of tokens (8000) specified by OpenAI. This will prevent server-side errors.\n",
    "\n",
    "def num_tokens_from_text(text: str, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Returns the number of OpenAI tokens.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "# Function to un-tokenize text\n",
    "def untokenize(tokens):\n",
    "    \"\"\"\n",
    "    Reverses the tokenization process.\n",
    "    \"\"\"\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply `untokenize`` function\n",
    "data['LemmatizedContent'] = data['LemmatizedContent'].apply(untokenize)\n",
    "\n",
    "# Calculate the token count\n",
    "data['TokenCount_LemmatizedContent'] = data['LemmatizedContent'].apply(num_tokens_from_text)\n",
    "\n",
    "# Count the number of reports that exceed the maximum number of tokens\n",
    "max_tokens = 8000\n",
    "exceeding_reports_count = (data['TokenCount_LemmatizedContent'] > max_tokens).sum()\n",
    "\n",
    "print(f\"Number of reports exceeding the maximum number of tokens: {exceeding_reports_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "      <th>TokenCount_LemmatizedContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>mr larsen year old male history mental health ...</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>process triaging prioritise ambulance attendan...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>question answer quora website provide informat...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>prison service instruction psi set procedure m...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>principal concern mental health patient miss r...</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>bart london whilst clear evidence heard inques...</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>piotr kucharz polish gentleman commence living...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>camden islington trust seem evidence heard cam...</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>strip cell door decease design engineer sketch...</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...   \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...   \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                LemmatizedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  mr larsen year old male history mental health ...   \n",
       "Ref: 2024-0311  process triaging prioritise ambulance attendan...   \n",
       "Ref: 2024-0298  question answer quora website provide informat...   \n",
       "Ref: 2024-0297  prison service instruction psi set procedure m...   \n",
       "Ref: 2024-0296  principal concern mental health patient miss r...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  bart london whilst clear evidence heard inques...   \n",
       "Ref: 2015-0465  piotr kucharz polish gentleman commence living...   \n",
       "Ref: 2015-0173  camden islington trust seem evidence heard cam...   \n",
       "Ref: 2015-0116  strip cell door decease design engineer sketch...   \n",
       "Ref: 2015-0072                                                      \n",
       "\n",
       "                TokenCount_LemmatizedContent  \n",
       "ID                                            \n",
       "Ref: 2024-0318                           706  \n",
       "Ref: 2024-0311                            72  \n",
       "Ref: 2024-0298                            98  \n",
       "Ref: 2024-0297                            85  \n",
       "Ref: 2024-0296                           501  \n",
       "...                                      ...  \n",
       "Ref: 2016-0037                           274  \n",
       "Ref: 2015-0465                           258  \n",
       "Ref: 2015-0173                           220  \n",
       "Ref: 2015-0116                           521  \n",
       "Ref: 2015-0072                             0  \n",
       "\n",
       "[415 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0 of 415\n",
      "Processing row 1 of 415\n",
      "Processing row 2 of 415\n",
      "Processing row 3 of 415\n",
      "Processing row 4 of 415\n",
      "Processing row 5 of 415\n",
      "Processing row 6 of 415\n",
      "Processing row 7 of 415\n",
      "Processing row 8 of 415\n",
      "Processing row 9 of 415\n",
      "Processing row 10 of 415\n",
      "Processing row 11 of 415\n",
      "Processing row 12 of 415\n",
      "Processing row 13 of 415\n",
      "Processing row 14 of 415\n",
      "Processing row 15 of 415\n",
      "Processing row 16 of 415\n",
      "Processing row 17 of 415\n",
      "Processing row 18 of 415\n",
      "Processing row 19 of 415\n",
      "Processing row 20 of 415\n",
      "Processing row 21 of 415\n",
      "Processing row 22 of 415\n",
      "Processing row 23 of 415\n",
      "Processing row 24 of 415\n",
      "Processing row 25 of 415\n",
      "Processing row 26 of 415\n",
      "Processing row 27 of 415\n",
      "Processing row 28 of 415\n",
      "Processing row 29 of 415\n",
      "Processing row 30 of 415\n",
      "Processing row 31 of 415\n",
      "Processing row 32 of 415\n",
      "Processing row 33 of 415\n",
      "Processing row 34 of 415\n",
      "Processing row 35 of 415\n",
      "Processing row 36 of 415\n",
      "Processing row 37 of 415\n",
      "Processing row 38 of 415\n",
      "Processing row 39 of 415\n",
      "Processing row 40 of 415\n",
      "Processing row 41 of 415\n",
      "Processing row 42 of 415\n",
      "Processing row 43 of 415\n",
      "Processing row 44 of 415\n",
      "Processing row 45 of 415\n",
      "Processing row 46 of 415\n",
      "Processing row 47 of 415\n",
      "Processing row 48 of 415\n",
      "Processing row 49 of 415\n",
      "Processing row 50 of 415\n",
      "Processing row 51 of 415\n",
      "Processing row 52 of 415\n",
      "Processing row 53 of 415\n",
      "Processing row 54 of 415\n",
      "Processing row 55 of 415\n",
      "Processing row 56 of 415\n",
      "Processing row 57 of 415\n",
      "Processing row 58 of 415\n",
      "Processing row 59 of 415\n",
      "Processing row 60 of 415\n",
      "Processing row 61 of 415\n",
      "Processing row 62 of 415\n",
      "Processing row 63 of 415\n",
      "Processing row 64 of 415\n",
      "Processing row 65 of 415\n",
      "Processing row 66 of 415\n",
      "Processing row 67 of 415\n",
      "Processing row 68 of 415\n",
      "Processing row 69 of 415\n",
      "Processing row 70 of 415\n",
      "Processing row 71 of 415\n",
      "Processing row 72 of 415\n",
      "Processing row 73 of 415\n",
      "Processing row 74 of 415\n",
      "Processing row 75 of 415\n",
      "Processing row 76 of 415\n",
      "Processing row 77 of 415\n",
      "Processing row 78 of 415\n",
      "Processing row 79 of 415\n",
      "Processing row 80 of 415\n",
      "Processing row 81 of 415\n",
      "Processing row 82 of 415\n",
      "Processing row 83 of 415\n",
      "Processing row 84 of 415\n",
      "Processing row 85 of 415\n",
      "Processing row 86 of 415\n",
      "Processing row 87 of 415\n",
      "Processing row 88 of 415\n",
      "Processing row 89 of 415\n",
      "Processing row 90 of 415\n",
      "Processing row 91 of 415\n",
      "Processing row 92 of 415\n",
      "Processing row 93 of 415\n",
      "Processing row 94 of 415\n",
      "Processing row 95 of 415\n",
      "Processing row 96 of 415\n",
      "Processing row 97 of 415\n",
      "Processing row 98 of 415\n",
      "Processing row 99 of 415\n",
      "Processing row 100 of 415\n",
      "Processing row 101 of 415\n",
      "Processing row 102 of 415\n",
      "Processing row 103 of 415\n",
      "Processing row 104 of 415\n",
      "Processing row 105 of 415\n",
      "Processing row 106 of 415\n",
      "Processing row 107 of 415\n",
      "Processing row 108 of 415\n",
      "Processing row 109 of 415\n",
      "Processing row 110 of 415\n",
      "Processing row 111 of 415\n",
      "Processing row 112 of 415\n",
      "Processing row 113 of 415\n",
      "Processing row 114 of 415\n",
      "Processing row 115 of 415\n",
      "Processing row 116 of 415\n",
      "Processing row 117 of 415\n",
      "Processing row 118 of 415\n",
      "Processing row 119 of 415\n",
      "Processing row 120 of 415\n",
      "Processing row 121 of 415\n",
      "Processing row 122 of 415\n",
      "Processing row 123 of 415\n",
      "Processing row 124 of 415\n",
      "Processing row 125 of 415\n",
      "Processing row 126 of 415\n",
      "Processing row 127 of 415\n",
      "Processing row 128 of 415\n",
      "Processing row 129 of 415\n",
      "Processing row 130 of 415\n",
      "Processing row 131 of 415\n",
      "Processing row 132 of 415\n",
      "Processing row 133 of 415\n",
      "Processing row 134 of 415\n",
      "Processing row 135 of 415\n",
      "Processing row 136 of 415\n",
      "Processing row 137 of 415\n",
      "Processing row 138 of 415\n",
      "Processing row 139 of 415\n",
      "Processing row 140 of 415\n",
      "Processing row 141 of 415\n",
      "Processing row 142 of 415\n",
      "Processing row 143 of 415\n",
      "Processing row 144 of 415\n",
      "Processing row 145 of 415\n",
      "Processing row 146 of 415\n",
      "Processing row 147 of 415\n",
      "Processing row 148 of 415\n",
      "Processing row 149 of 415\n",
      "Processing row 150 of 415\n",
      "Processing row 151 of 415\n",
      "Processing row 152 of 415\n",
      "Processing row 153 of 415\n",
      "Processing row 154 of 415\n",
      "Processing row 155 of 415\n",
      "Processing row 156 of 415\n",
      "Processing row 157 of 415\n",
      "Processing row 158 of 415\n",
      "Processing row 159 of 415\n",
      "Processing row 160 of 415\n",
      "Processing row 161 of 415\n",
      "Processing row 162 of 415\n",
      "Processing row 163 of 415\n",
      "Processing row 164 of 415\n",
      "Processing row 165 of 415\n",
      "Processing row 166 of 415\n",
      "Processing row 167 of 415\n",
      "Processing row 168 of 415\n",
      "Processing row 169 of 415\n",
      "Processing row 170 of 415\n",
      "Processing row 171 of 415\n",
      "Processing row 172 of 415\n",
      "Processing row 173 of 415\n",
      "Processing row 174 of 415\n",
      "Error on row 174: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 175 of 415\n",
      "Processing row 176 of 415\n",
      "Processing row 177 of 415\n",
      "Processing row 178 of 415\n",
      "Processing row 179 of 415\n",
      "Processing row 180 of 415\n",
      "Processing row 181 of 415\n",
      "Processing row 182 of 415\n",
      "Processing row 183 of 415\n",
      "Processing row 184 of 415\n",
      "Processing row 185 of 415\n",
      "Processing row 186 of 415\n",
      "Processing row 187 of 415\n",
      "Processing row 188 of 415\n",
      "Processing row 189 of 415\n",
      "Processing row 190 of 415\n",
      "Processing row 191 of 415\n",
      "Processing row 192 of 415\n",
      "Processing row 193 of 415\n",
      "Processing row 194 of 415\n",
      "Processing row 195 of 415\n",
      "Processing row 196 of 415\n",
      "Processing row 197 of 415\n",
      "Processing row 198 of 415\n",
      "Processing row 199 of 415\n",
      "Processing row 200 of 415\n",
      "Processing row 201 of 415\n",
      "Processing row 202 of 415\n",
      "Processing row 203 of 415\n",
      "Processing row 204 of 415\n",
      "Processing row 205 of 415\n",
      "Processing row 206 of 415\n",
      "Processing row 207 of 415\n",
      "Processing row 208 of 415\n",
      "Processing row 209 of 415\n",
      "Processing row 210 of 415\n",
      "Processing row 211 of 415\n",
      "Processing row 212 of 415\n",
      "Processing row 213 of 415\n",
      "Processing row 214 of 415\n",
      "Processing row 215 of 415\n",
      "Processing row 216 of 415\n",
      "Processing row 217 of 415\n",
      "Processing row 218 of 415\n",
      "Processing row 219 of 415\n",
      "Processing row 220 of 415\n",
      "Processing row 221 of 415\n",
      "Processing row 222 of 415\n",
      "Processing row 223 of 415\n",
      "Processing row 224 of 415\n",
      "Processing row 225 of 415\n",
      "Processing row 226 of 415\n",
      "Processing row 227 of 415\n",
      "Processing row 228 of 415\n",
      "Processing row 229 of 415\n",
      "Processing row 230 of 415\n",
      "Processing row 231 of 415\n",
      "Processing row 232 of 415\n",
      "Processing row 233 of 415\n",
      "Processing row 234 of 415\n",
      "Processing row 235 of 415\n",
      "Processing row 236 of 415\n",
      "Processing row 237 of 415\n",
      "Processing row 238 of 415\n",
      "Processing row 239 of 415\n",
      "Processing row 240 of 415\n",
      "Processing row 241 of 415\n",
      "Processing row 242 of 415\n",
      "Processing row 243 of 415\n",
      "Processing row 244 of 415\n",
      "Processing row 245 of 415\n",
      "Processing row 246 of 415\n",
      "Processing row 247 of 415\n",
      "Processing row 248 of 415\n",
      "Processing row 249 of 415\n",
      "Processing row 250 of 415\n",
      "Processing row 251 of 415\n",
      "Processing row 252 of 415\n",
      "Processing row 253 of 415\n",
      "Processing row 254 of 415\n",
      "Processing row 255 of 415\n",
      "Processing row 256 of 415\n",
      "Processing row 257 of 415\n",
      "Processing row 258 of 415\n",
      "Processing row 259 of 415\n",
      "Processing row 260 of 415\n",
      "Processing row 261 of 415\n",
      "Processing row 262 of 415\n",
      "Processing row 263 of 415\n",
      "Processing row 264 of 415\n",
      "Processing row 265 of 415\n",
      "Processing row 266 of 415\n",
      "Processing row 267 of 415\n",
      "Processing row 268 of 415\n",
      "Processing row 269 of 415\n",
      "Processing row 270 of 415\n",
      "Processing row 271 of 415\n",
      "Processing row 272 of 415\n",
      "Processing row 273 of 415\n",
      "Processing row 274 of 415\n",
      "Processing row 275 of 415\n",
      "Processing row 276 of 415\n",
      "Processing row 277 of 415\n",
      "Processing row 278 of 415\n",
      "Processing row 279 of 415\n",
      "Processing row 280 of 415\n",
      "Processing row 281 of 415\n",
      "Processing row 282 of 415\n",
      "Error on row 282: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 283 of 415\n",
      "Processing row 284 of 415\n",
      "Processing row 285 of 415\n",
      "Error on row 285: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 286 of 415\n",
      "Processing row 287 of 415\n",
      "Processing row 288 of 415\n",
      "Processing row 289 of 415\n",
      "Processing row 290 of 415\n",
      "Processing row 291 of 415\n",
      "Processing row 292 of 415\n",
      "Error on row 292: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 293 of 415\n",
      "Processing row 294 of 415\n",
      "Processing row 295 of 415\n",
      "Processing row 296 of 415\n",
      "Error on row 296: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 297 of 415\n",
      "Processing row 298 of 415\n",
      "Processing row 299 of 415\n",
      "Error on row 299: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 300 of 415\n",
      "Processing row 301 of 415\n",
      "Processing row 302 of 415\n",
      "Processing row 303 of 415\n",
      "Error on row 303: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 304 of 415\n",
      "Processing row 305 of 415\n",
      "Processing row 306 of 415\n",
      "Processing row 307 of 415\n",
      "Processing row 308 of 415\n",
      "Processing row 309 of 415\n",
      "Processing row 310 of 415\n",
      "Processing row 311 of 415\n",
      "Processing row 312 of 415\n",
      "Processing row 313 of 415\n",
      "Processing row 314 of 415\n",
      "Processing row 315 of 415\n",
      "Processing row 316 of 415\n",
      "Processing row 317 of 415\n",
      "Processing row 318 of 415\n",
      "Processing row 319 of 415\n",
      "Processing row 320 of 415\n",
      "Processing row 321 of 415\n",
      "Processing row 322 of 415\n",
      "Processing row 323 of 415\n",
      "Processing row 324 of 415\n",
      "Processing row 325 of 415\n",
      "Processing row 326 of 415\n",
      "Processing row 327 of 415\n",
      "Processing row 328 of 415\n",
      "Processing row 329 of 415\n",
      "Processing row 330 of 415\n",
      "Processing row 331 of 415\n",
      "Processing row 332 of 415\n",
      "Processing row 333 of 415\n",
      "Processing row 334 of 415\n",
      "Error on row 334: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 335 of 415\n",
      "Processing row 336 of 415\n",
      "Processing row 337 of 415\n",
      "Processing row 338 of 415\n",
      "Processing row 339 of 415\n",
      "Processing row 340 of 415\n",
      "Processing row 341 of 415\n",
      "Processing row 342 of 415\n",
      "Processing row 343 of 415\n",
      "Processing row 344 of 415\n",
      "Processing row 345 of 415\n",
      "Error on row 345: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 346 of 415\n",
      "Processing row 347 of 415\n",
      "Processing row 348 of 415\n",
      "Processing row 349 of 415\n",
      "Processing row 350 of 415\n",
      "Processing row 351 of 415\n",
      "Processing row 352 of 415\n",
      "Processing row 353 of 415\n",
      "Processing row 354 of 415\n",
      "Processing row 355 of 415\n",
      "Processing row 356 of 415\n",
      "Processing row 357 of 415\n",
      "Processing row 358 of 415\n",
      "Error on row 358: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 359 of 415\n",
      "Processing row 360 of 415\n",
      "Error on row 360: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 361 of 415\n",
      "Processing row 362 of 415\n",
      "Processing row 363 of 415\n",
      "Processing row 364 of 415\n",
      "Processing row 365 of 415\n",
      "Error on row 365: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 366 of 415\n",
      "Processing row 367 of 415\n",
      "Processing row 368 of 415\n",
      "Processing row 369 of 415\n",
      "Processing row 370 of 415\n",
      "Processing row 371 of 415\n",
      "Processing row 372 of 415\n",
      "Processing row 373 of 415\n",
      "Processing row 374 of 415\n",
      "Processing row 375 of 415\n",
      "Processing row 376 of 415\n",
      "Processing row 377 of 415\n",
      "Processing row 378 of 415\n",
      "Processing row 379 of 415\n",
      "Processing row 380 of 415\n",
      "Processing row 381 of 415\n",
      "Error on row 381: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 382 of 415\n",
      "Error on row 382: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 383 of 415\n",
      "Error on row 383: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 384 of 415\n",
      "Processing row 385 of 415\n",
      "Processing row 386 of 415\n",
      "Processing row 387 of 415\n",
      "Processing row 388 of 415\n",
      "Processing row 389 of 415\n",
      "Processing row 390 of 415\n",
      "Processing row 391 of 415\n",
      "Processing row 392 of 415\n",
      "Error on row 392: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Processing row 393 of 415\n",
      "Processing row 394 of 415\n",
      "Processing row 395 of 415\n",
      "Processing row 396 of 415\n",
      "Processing row 397 of 415\n",
      "Processing row 398 of 415\n",
      "Processing row 399 of 415\n",
      "Processing row 400 of 415\n",
      "Processing row 401 of 415\n",
      "Processing row 402 of 415\n",
      "Processing row 403 of 415\n",
      "Processing row 404 of 415\n",
      "Processing row 405 of 415\n",
      "Processing row 406 of 415\n",
      "Processing row 407 of 415\n",
      "Processing row 408 of 415\n",
      "Processing row 409 of 415\n",
      "Processing row 410 of 415\n",
      "Processing row 411 of 415\n",
      "Processing row 412 of 415\n",
      "Processing row 413 of 415\n",
      "Processing row 414 of 415\n",
      "Error on row 414: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Time taken: 3 minutes and 6.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "      <th>TokenCount_LemmatizedContent</th>\n",
       "      <th>text-embedding-3-large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ref: 2024-0318</td>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>mr larsen year old male history mental health ...</td>\n",
       "      <td>706</td>\n",
       "      <td>[0.03439606726169586, 0.0004933655727654696, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ref: 2024-0311</td>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>process triaging prioritise ambulance attendan...</td>\n",
       "      <td>72</td>\n",
       "      <td>[0.015928048640489578, 0.015428761951625347, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ref: 2024-0298</td>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>question answer quora website provide informat...</td>\n",
       "      <td>98</td>\n",
       "      <td>[-0.026135534048080444, -0.027413271367549896,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ref: 2024-0297</td>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>prison service instruction psi set procedure m...</td>\n",
       "      <td>85</td>\n",
       "      <td>[0.030314801260828972, 0.003587251529097557, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ref: 2024-0296</td>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>principal concern mental health patient miss r...</td>\n",
       "      <td>501</td>\n",
       "      <td>[0.04845774546265602, 5.705011062673293e-05, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>Ref: 2016-0037</td>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>bart london whilst clear evidence heard inques...</td>\n",
       "      <td>274</td>\n",
       "      <td>[0.030965100973844528, 0.026973573490977287, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>Ref: 2015-0465</td>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>piotr kucharz polish gentleman commence living...</td>\n",
       "      <td>258</td>\n",
       "      <td>[-0.0017374138114973903, 0.011945893988013268,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>412</td>\n",
       "      <td>Ref: 2015-0173</td>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>camden islington trust seem evidence heard cam...</td>\n",
       "      <td>220</td>\n",
       "      <td>[0.019967302680015564, 0.007394141983240843, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>413</td>\n",
       "      <td>Ref: 2015-0116</td>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>strip cell door decease design engineer sketch...</td>\n",
       "      <td>521</td>\n",
       "      <td>[0.02099669724702835, 0.00478058448061347, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>414</td>\n",
       "      <td>Ref: 2015-0072</td>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index              ID                                       CleanContent  \\\n",
       "0        0  Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "1        1  Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "2        2  Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "3        3  Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "4        4  Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "..     ...             ...                                                ...   \n",
       "410    410  Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "411    411  Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "412    412  Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "413    413  Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "414    414  Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                      ProcessedContent  \\\n",
       "0    [mr, larsen, year, old, male, history, mental,...   \n",
       "1    [process, triaging, prioritising, ambulance, a...   \n",
       "2    [questions, answers, quora, website, provide, ...   \n",
       "3    [prison, service, instruction, psi, sets, proc...   \n",
       "4    [principal, concern, mental, health, patient, ...   \n",
       "..                                                 ...   \n",
       "410  [barts, london, whilst, clear, evidence, heard...   \n",
       "411  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "412  [camden, islington, trust, seemed, evidence, h...   \n",
       "413  [strips, cell, doors, deceased, design, engine...   \n",
       "414                                                 []   \n",
       "\n",
       "                                     LemmatizedContent  \\\n",
       "0    mr larsen year old male history mental health ...   \n",
       "1    process triaging prioritise ambulance attendan...   \n",
       "2    question answer quora website provide informat...   \n",
       "3    prison service instruction psi set procedure m...   \n",
       "4    principal concern mental health patient miss r...   \n",
       "..                                                 ...   \n",
       "410  bart london whilst clear evidence heard inques...   \n",
       "411  piotr kucharz polish gentleman commence living...   \n",
       "412  camden islington trust seem evidence heard cam...   \n",
       "413  strip cell door decease design engineer sketch...   \n",
       "414                                                      \n",
       "\n",
       "     TokenCount_LemmatizedContent  \\\n",
       "0                             706   \n",
       "1                              72   \n",
       "2                              98   \n",
       "3                              85   \n",
       "4                             501   \n",
       "..                            ...   \n",
       "410                           274   \n",
       "411                           258   \n",
       "412                           220   \n",
       "413                           521   \n",
       "414                             0   \n",
       "\n",
       "                                text-embedding-3-large  \n",
       "0    [0.03439606726169586, 0.0004933655727654696, -...  \n",
       "1    [0.015928048640489578, 0.015428761951625347, 0...  \n",
       "2    [-0.026135534048080444, -0.027413271367549896,...  \n",
       "3    [0.030314801260828972, 0.003587251529097557, -...  \n",
       "4    [0.04845774546265602, 5.705011062673293e-05, -...  \n",
       "..                                                 ...  \n",
       "410  [0.030965100973844528, 0.026973573490977287, -...  \n",
       "411  [-0.0017374138114973903, 0.011945893988013268,...  \n",
       "412  [0.019967302680015564, 0.007394141983240843, -...  \n",
       "413  [0.02099669724702835, 0.00478058448061347, -0....  \n",
       "414                                               None  \n",
       "\n",
       "[415 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up OpenAI API\n",
    "load_dotenv('api.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Create function that provides text embeddings\n",
    "def get_embedding(text_to_embbed, model_ID):\n",
    "    text = text_to_embbed.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text_to_embbed], model=model_ID).data[0].embedding\n",
    "\n",
    "# Define empty list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Reset index to avoid bug\n",
    "data = data.reset_index(drop=False)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over each element of \"LemmatizedContent\" field and get embeddings\n",
    "for idx, text in enumerate(data['LemmatizedContent']):\n",
    "    print('Processing row {i} of {n}'.format(i=idx, n=len(data)))\n",
    "    try:\n",
    "        embedding = get_embedding(text, \"text-embedding-3-large\")\n",
    "        embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        embeddings.append(None)\n",
    "        print(f'Error on row {idx}: {e}')\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')\n",
    "\n",
    "# Add embeddings to the dataframe\n",
    "data['text-embedding-3-large'] = embeddings\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the above broadly worked, but the console is printing lots of errors. We can investigate these individual reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                                                                         174\n",
      "ID                              Date of report: 12/01/2023Ref: 2023-0015Deceas...\n",
      "CleanContent                                                                   \"\"\n",
      "ProcessedContent                                                               []\n",
      "LemmatizedContent                                                                \n",
      "TokenCount_LemmatizedContent                                                    0\n",
      "text-embedding-3-large                                                       None\n",
      "Name: 174, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print row 174 in \"data\"\n",
    "print(data.loc[174])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these errors exist because our the could not remove the intro text in `preprocess.ipynb`, and provided an empty string. This is fine for now, as the number of errors is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Retokenise the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatizedContent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLemmatizedContent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m~/miniconda3/envs/PFD/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Retokenise the data\n",
    "data['LemmatizedContent'] = data['LemmatizedContent'].apply(word_tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589 3072\n"
     ]
    }
   ],
   "source": [
    "print(len(data['LemmatizedContent'][0]), len(data['text-embedding-3-large'][0]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and process data\n",
    "\n",
    "Before topic modelling, we need to: (1) tokenize the data; (2) remove punctuation, special characters and numbers; (3) remove stop words; (4) lemmatize tokens to their dictionary base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenise the report content\n",
    "data['TokenisedContent'] = data['CleanContent'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation, special characters and numbers\n",
    "data['TokenisedContent'] = data['TokenisedContent'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['TokenisedContent'] = data['TokenisedContent'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row of \"data\" with ID of \"Ref: 2015-0072\" due to erronous output\n",
    "# This is a temporary fix. Some prompt engineering is needed in the OpenAI API call to prevent erronous outputs\n",
    "data = data.drop('Ref: 2015-0072')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
