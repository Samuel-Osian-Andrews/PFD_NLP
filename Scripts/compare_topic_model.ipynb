{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing topic modelling techniques\n",
    "\n",
    "There are different topic modelling approaches, each with a different set of advantages and disadvantages. The 'best' modelling technique is far from absolute, and largely depends on the nuances of the text data being analysed. To our knowledge, PFD data has not yet been analysed via NLP or topic modelling techniques, meaning that there exists no literature on the optimal approach(es).\n",
    "\n",
    "This notebook will compare the suitability of 5 topic modelling techniques for PFD 'concerns' data.<br><br><br>\n",
    "\n",
    "\n",
    "1. **Latent Dirichlet Allocation (LDA)**\n",
    "\n",
    "LDA is perhaps the most popular topic modelling technique. It is a probabilistic method that assumes each document is a mixture of various topics (likely suitable for PFD reports which frequently contain multiple concerns). It characterises topics as a 'mixture of words'; the model generates a topic distribution for each document and a word distribution for each topic.\n",
    "\n",
    "LDA *does* require that we pre-define our number of topics.\n",
    "\n",
    "It uses Dirichlet distribution priors to model the distribution of topics in documents and words in topics, providing a more statistically aligned framework for topic modelling.<br><br><br>\n",
    "\n",
    "\n",
    "2. **Correlated Topic Modelling (CTM)**\n",
    "\n",
    "CTM is an extension of LDA that allows for correlations between topics. While it carries over core disadvantages of LDA in terms of less interpretable keyword lists for each topic, its unique contribution is its inclusion of a covariance structure to model topic correlations. This is particularly interesting for our PFD data, where many reports are built from multiple concerns and therefore topics. \n",
    "\n",
    "CTM *does* require us to pre-define our number of topics.<br><br><br>\n",
    "\n",
    "\n",
    "3. **Non-negative Matrix Factorisation (NMF)**\n",
    "\n",
    "NMF is a matrix factorisation technique that decomposes the document-term matrix into two lower-dimensional matrices. Topics are characterised by non-negative components in the factorised matrices, representing the importance of words in topics and topics in documents. Similarly to LDA, it assumes that documents contain multiple topics.\n",
    "\n",
    "NMF *does* require that we pre-define our number of topics.\n",
    "\n",
    "NMF enforces non-negativity constraints. Many report that resulting topic keywords are therefore more interpretable than LDA, with less 'noise' in the keyword lists.<br><br><br>\n",
    "\n",
    "\n",
    "4. **Top2Vec**\n",
    "\n",
    "Topics in Top2Vec are characterised by dense clusters of document and word embeddings. These clusters are identified in a joint embedding space, where both documents and words are represented. It does allow for multiple topics per document; this is achieved through the proximity of document embeddings to multiple topic vectors in the semantic space.\n",
    "\n",
    "Top2Vec does *not* require us to pre-define our number of topics.\n",
    "\n",
    "Top2Vec uses deep learning-based embeddings (e.g., Doc2Vec, Universal Sentence Encoder) to capture the semantic relationships in the text. This method ensures that topics are discovered based on the natural clustering of similar documents and words, leading to a more intuitive and data-driven identification of topics.<br><br><br>\n",
    "\n",
    "\n",
    "5. **BERTopic**\n",
    "\n",
    "BERTopic uses BERT embeddings and clustering algorithms to discover topics. Topics are characterised by dense clusters of semantically similar embeddings, identified through dimensionality reduction and clustering. Although not originally supported, v0.13 (January 2023) allows us to approximate a probabilistic topic distribution for each report via '.approximate_distribution'.\n",
    "\n",
    "BERTopic does *not* require us to pre-define our number of topics.<br><br>\n",
    "\n",
    "\n",
    "### Before we get started\n",
    "\n",
    "There are a few processing steps we need to complete before running this comparison. We will remove stop words, punctuation and numbers from our report content, lemmatize the text, and perform word embeddings.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent\n",
       "ID                                                               \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...\n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...\n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...\n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...\n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...\n",
       "...                                                           ...\n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...\n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...\n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...\n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...\n",
       "Ref: 2015-0072                                                 \"\"\n",
       "\n",
       "[415 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import cleaned data\n",
    "data = pd.read_csv('../Data/cleaned.csv', index_col='ID')\n",
    "\n",
    "# Just keep \"CleanContent\" field\n",
    "data = data[['CleanContent']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuation \n",
    "\n",
    "This stage is vital for removing words like \"the\" and \"my\" from the reports. These words provide unnecessary 'noise' in topic modelling which can result in much less coherent topics.\n",
    "\n",
    "Below, we tokenise the report content and remove stop words, numbers and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...  \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...  \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...  \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...  \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...  \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...  \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...  \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...  \n",
       "Ref: 2015-0072                                                 []  \n",
       "\n",
       "[415 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the report content\n",
    "data['ProcessedContent'] = data['CleanContent'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation, special characters, and numbers, and convert to lowercase\n",
    "data['ProcessedContent'] = data['ProcessedContent'].apply(lambda x: [word.lower() for word in x if word.isalpha()])\n",
    "\n",
    "# Remove stopwords\n",
    "data['ProcessedContent'] = data['ProcessedContent'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the data\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form. For example, the words \"running\", \"runs\" and \"ran\" all need to be returned to their base form of \"run\".\n",
    "\n",
    "Lemmatization is generally favourable to 'stemming' because the former returns a semantically meaningful output. For example, stemming would return \"better\" as \"bet\" while lemmatization would return it as \"good\".\n",
    "\n",
    "We can also enhance this process via 'part-of-speech' (POS) tagging. POS tagging enhances lemmatization by identifying the grammatical role of words. For example, without POS tagging, the word \"lead\" in the sentences \"The lead levels were high\" and \"He will lead the investigation\" could be incorrectly lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map POS tags for lemmatization\n",
    "# ...J = Adjective, R = Adverb, V = Verb, N = Noun\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>[process, triaging, prioritise, ambulance, att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...   \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...   \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                LemmatizedContent  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...  \n",
       "Ref: 2024-0311  [process, triaging, prioritise, ambulance, att...  \n",
       "Ref: 2024-0298  [question, answer, quora, website, provide, in...  \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, set, proce...  \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [bart, london, whilst, clear, evidence, heard,...  \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commence, ...  \n",
       "Ref: 2015-0173  [camden, islington, trust, seem, evidence, hea...  \n",
       "Ref: 2015-0116  [strip, cell, door, decease, design, engineer,...  \n",
       "Ref: 2015-0072                                                 []  \n",
       "\n",
       "[415 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Initialise the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to process tokens\n",
    "def process_content(tokens):\n",
    "    try:\n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Lemmatize with POS tags\n",
    "        lemmatized_tokens = []\n",
    "        for token, tag in pos_tags:\n",
    "            wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "        \n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the process_content function\n",
    "data['LemmatizedContent'] = data['ProcessedContent'].apply(process_content)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "It's useful to use word embeddings prior to topic modelling in order to capture semantic similarity between certain words. For example, words like 'medicine', 'drugs' and 'prescription' would all be treated independently if we did not use embeddings, despite them potentially having similar meanings. \n",
    "\n",
    "By using word embeddings, we numerically link words with similar usage contexts and therefore increase the chances of our topic models presenting more coherent topics.\n",
    "\n",
    "Below we use a pre-trained Word2Vec model from Gensim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0 minutes and 26.97 seconds\n",
      "Out-of-vocabulary words:\n",
      "cbt\n",
      "bmshft\n",
      "marf\n",
      "findlay\n",
      "organise\n",
      "mercia\n",
      "marac\n",
      "wearent\n",
      "lul\n",
      "programme\n",
      "lrfaan\n",
      "homerton\n",
      "callie\n",
      "waverley\n",
      "dyfed\n",
      "cuous\n",
      "snanothing\n",
      "chelmer\n",
      "usefu\n",
      "theistpital\n",
      "tamiem\n",
      "kieran\n",
      "cumbria\n",
      "swp\n",
      "systmone\n",
      "cwp\n",
      "prioritise\n",
      "ehcps\n",
      "tirgari\n",
      "hwhct\n",
      "wwr\n",
      "whinhi\n",
      "chanthirakumar\n",
      "warrn\n",
      "hmrc\n",
      "characterise\n",
      "buckett\n",
      "leicestershire\n",
      "hillingdon\n",
      "costello\n",
      "lpmhss\n",
      "heathcotes\n",
      "templeton\n",
      "agnès\n",
      "davy\n",
      "cmht\n",
      "finalise\n",
      "bcuhb\n",
      "complianceon\n",
      "mustafa\n",
      "rca\n",
      "thereoffony\n",
      "loworum\n",
      "salford\n",
      "enfield\n",
      "eput\n",
      "carlon\n",
      "oasys\n",
      "oacmht\n",
      "hmac\n",
      "ipts\n",
      "summarising\n",
      "depaul\n",
      "quidance\n",
      "hbtt\n",
      "cmhts\n",
      "cqc\n",
      "stockport\n",
      "oskar\n",
      "ihcbtt\n",
      "metcalfe\n",
      "broadland\n",
      "grosvenor\n",
      "mlght\n",
      "sodexo\n",
      "rbc\n",
      "eeast\n",
      "orxsat\n",
      "azra\n",
      "irene\n",
      "katharine\n",
      "spoaa\n",
      "northumbria\n",
      "doncaster\n",
      "behavioural\n",
      "ghct\n",
      "practise\n",
      "meghan\n",
      "caremap\n",
      "abuhb\n",
      "northgate\n",
      "gwynedd\n",
      "organisation\n",
      "authorise\n",
      "rlb\n",
      "dhr\n",
      "prbpranolol\n",
      "cortel\n",
      "fnos\n",
      "chrinage\n",
      "atherleigh\n",
      "condron\n",
      "ania\n",
      "regiriie\n",
      "outsettionit\n",
      "gmmh\n",
      "powys\n",
      "materialise\n",
      "realise\n",
      "perrott\n",
      "relationist\n",
      "rhian\n",
      "bedfordshire\n",
      "lancashire\n",
      "cambs\n",
      "rationalisation\n",
      "neurodivergent\n",
      "practician\n",
      "calibre\n",
      "geraint\n",
      "acknowledgement\n",
      "clements\n",
      "arezou\n",
      "browne\n",
      "peterborough\n",
      "bsarc\n",
      "ccgs\n",
      "chrtt\n",
      "waddup\n",
      "consequetnly\n",
      "psychiatristconsultations\n",
      "forcumbria\n",
      "whittington\n",
      "oposes\n",
      "jlmping\n",
      "ravenswood\n",
      "claires\n",
      "islington\n",
      "offence\n",
      "accadenic\n",
      "icd\n",
      "zarins\n",
      "elmley\n",
      "medway\n",
      "camhs\n",
      "portchester\n",
      "marnie\n",
      "northumberland\n",
      "honnor\n",
      "hhj\n",
      "learni\n",
      "ligaturing\n",
      "lowther\n",
      "bdd\n",
      "swann\n",
      "ambubag\n",
      "immedialely\n",
      "amph\n",
      "eifion\n",
      "britton\n",
      "chenfrom\n",
      "speciality\n",
      "meagre\n",
      "roner\n",
      "miriam\n",
      "hmp\n",
      "wokingham\n",
      "lewes\n",
      "notifiesogical\n",
      "whitchurch\n",
      "covid\n",
      "larsen\n",
      "longstaff\n",
      "fulwood\n",
      "tobias\n",
      "odourless\n",
      "hwlffordd\n",
      "impend\n",
      "freeda\n",
      "ucas\n",
      "pcr\n",
      "hmpps\n",
      "topsham\n",
      "wcbc\n",
      "viswambaran\n",
      "bch\n",
      "bsmhft\n",
      "belmarsh\n",
      "npcc\n",
      "dependant\n",
      "launceston\n",
      "shirehall\n",
      "derbyshire\n",
      "clds\n",
      "counsellor\n",
      "hca\n",
      "wilh\n",
      "jacobson\n",
      "monitorinq\n",
      "personalise\n",
      "xuanze\n",
      "nottinghamshire\n",
      "gingell\n",
      "mclellan\n",
      "bevan\n",
      "psii\n",
      "manston\n",
      "sbp\n",
      "neighbouring\n",
      "chiet\n",
      "qtlb\n",
      "ofcom\n",
      "aylesbury\n",
      "ehcp\n",
      "septpt\n",
      "iewg\n",
      "stepps\n",
      "lldistpokesmaid\n",
      "bourton\n",
      "candour\n",
      "neurodiverse\n",
      "piotr\n",
      "utilise\n",
      "paediatric\n",
      "hywel\n",
      "walczak\n",
      "initaitves\n",
      "langley\n",
      "guldance\n",
      "providethe\n",
      "dlss\n",
      "behaviours\n",
      "neurodiverity\n",
      "stillnot\n",
      "hewell\n",
      "chguidance\n",
      "tewvs\n",
      "vou\n",
      "gof\n",
      "tostevin\n",
      "neighbour\n",
      "ofcialtthe\n",
      "gsz\n",
      "tameside\n",
      "lyalushko\n",
      "nomis\n",
      "icb\n",
      "luton\n",
      "rco\n",
      "counselling\n",
      "gnific\n",
      "rowley\n",
      "suooort\n",
      "mitigatethis\n",
      "endeavour\n",
      "winchester\n",
      "mollie\n",
      "unfavourably\n",
      "catharine\n",
      "rsm\n",
      "hpcs\n",
      "guildford\n",
      "mqht\n",
      "pimlott\n",
      "dapdune\n",
      "reiver\n",
      "onse\n",
      "mdts\n",
      "osg\n",
      "unauthorised\n",
      "moj\n",
      "formalise\n",
      "unrecognised\n",
      "utilising\n",
      "corfax\n",
      "mrwaddup\n",
      "tooverride\n",
      "guildowns\n",
      "caernarfon\n",
      "hadrian\n",
      "forryan\n",
      "minimise\n",
      "labour\n",
      "wherethere\n",
      "waveney\n",
      "baynham\n",
      "cjldt\n",
      "dando\n",
      "pco\n",
      "referralsvia\n",
      "bede\n",
      "boughton\n",
      "ipp\n",
      "coraddress\n",
      "chipperfield\n",
      "elft\n",
      "wyre\n",
      "spoe\n",
      "gibbins\n",
      "llidiard\n",
      "hmpss\n",
      "datix\n",
      "wedgwood\n",
      "apologise\n",
      "lokeman\n",
      "irj\n",
      "poole\n",
      "walsall\n",
      "ssab\n",
      "chrismas\n",
      "oxleas\n",
      "actionless\n",
      "vincenzo\n",
      "systemone\n",
      "aubyn\n",
      "rdos\n",
      "lgbtqia\n",
      "jeopardise\n",
      "ddas\n",
      "brittain\n",
      "behaviour\n",
      "ligate\n",
      "chiefcoronersoffice\n",
      "judgement\n",
      "clinicans\n",
      "lorenzo\n",
      "crisisteam\n",
      "knownligature\n",
      "thameslink\n",
      "merseyside\n",
      "eeas\n",
      "houseblock\n",
      "scrutinised\n",
      "wedgewood\n",
      "aneurin\n",
      "cramlington\n",
      "kenneway\n",
      "kirsty\n",
      "plenrece\n",
      "cytno\n",
      "neas\n",
      "enquiry\n",
      "minueting\n",
      "rmn\n",
      "bpas\n",
      "chelmsford\n",
      "vauxhall\n",
      "mindworks\n",
      "grey\n",
      "lsotretinion\n",
      "ihbtt\n",
      "scrutinise\n",
      "csm\n",
      "robyn\n",
      "idt\n",
      "hcas\n",
      "mpokesman\n",
      "turbutt\n",
      "notest\n",
      "kirkham\n",
      "zuclopenthixol\n",
      "amhp\n",
      "gartree\n",
      "bampfylde\n",
      "sajid\n",
      "recognised\n",
      "millview\n",
      "gwent\n",
      "dundhal\n",
      "ocd\n",
      "snanoakes\n",
      "risley\n",
      "anglia\n",
      "protectionistmental\n",
      "imb\n",
      "adaptthe\n",
      "gtrust\n",
      "ibelieve\n",
      "dwp\n",
      "darrell\n",
      "perfectthe\n",
      "certifv\n",
      "unravelling\n",
      "underdevelop\n",
      "rutland\n",
      "ledto\n",
      "clopixol\n",
      "eetoffony\n",
      "bsarcs\n",
      "mcloughlin\n",
      "emphasise\n",
      "teague\n",
      "chagtaltyposeuf\n",
      "dickinson\n",
      "manoeuvre\n",
      "stevyn\n",
      "bsmht\n",
      "sacu\n",
      "mav\n",
      "moyce\n",
      "goodliffe\n",
      "reorganise\n",
      "communbation\n",
      "pentonville\n",
      "radica\n",
      "elli\n",
      "ofsted\n",
      "glausiusz\n",
      "barnaby\n",
      "kernow\n",
      "iapt\n",
      "suffiently\n",
      "cte\n",
      "criticise\n",
      "pfds\n",
      "gawthorpe\n",
      "amitryptyline\n",
      "hinton\n",
      "lch\n",
      "clinica\n",
      "sii\n",
      "loughborough\n",
      "crhtt\n",
      "prh\n",
      "functus\n",
      "upvoting\n",
      "forchecking\n",
      "secambs\n",
      "shere\n",
      "standardised\n",
      "duignan\n",
      "fno\n",
      "stort\n",
      "centralise\n",
      "recognise\n",
      "navigo\n",
      "whitelaw\n",
      "warwickshire\n",
      "mias\n",
      "keyworkers\n",
      "haverigg\n",
      "bardoliwalla\n",
      "frimley\n",
      "treatmentt\n",
      "belgravia\n",
      "minimisation\n",
      "icope\n",
      "siwan\n",
      "jaden\n",
      "parkwood\n",
      "kmpt\n",
      "nadia\n",
      "wiltshire\n",
      "pinderfields\n",
      "oxevision\n",
      "anaesthetic\n",
      "thististley\n",
      "particularise\n",
      "addaction\n",
      "dithe\n",
      "dcmh\n",
      "standardise\n",
      "outernet\n",
      "iow\n",
      "gids\n",
      "organisational\n",
      "guedalla\n",
      "enteriing\n",
      "cmhrs\n",
      "anv\n",
      "ctmuhb\n",
      "licence\n",
      "optimise\n",
      "talygarn\n",
      "humberside\n",
      "malone\n",
      "cnps\n",
      "prescriberistatus\n",
      "oranisations\n",
      "reece\n",
      "oic\n",
      "randomised\n",
      "ashortage\n",
      "pennine\n",
      "approash\n",
      "exsacerbated\n",
      "undertakenen\n",
      "ecome\n",
      "haringey\n",
      "kaye\n",
      "ruthin\n",
      "cpns\n",
      "wmp\n",
      "haverfordwest\n",
      "sychological\n",
      "cambridgeshire\n",
      "nhct\n",
      "practictioner\n",
      "neuadd\n",
      "enquire\n",
      "nitrazistpam\n",
      "esca\n",
      "horstead\n",
      "nsft\n",
      "lfb\n",
      "thisreport\n",
      "cobain\n",
      "morrill\n",
      "dovegate\n",
      "tewv\n",
      "trainina\n",
      "rvant\n",
      "rpfd\n",
      "mcmanus\n",
      "rebreathe\n",
      "camh\n",
      "haik\n",
      "undy\n",
      "pfdr\n",
      "immigrationn\n",
      "ofrelapse\n",
      "waterhouse\n",
      "wyndham\n",
      "spocs\n",
      "woodberry\n",
      "pontypool\n",
      "pembrokeshire\n",
      "rnna\n",
      "plenw\n",
      "assessers\n",
      "authorisation\n",
      "kucharz\n",
      "pontypridd\n",
      "grayshott\n",
      "amhps\n",
      "patien\n",
      "eupd\n",
      "lpt\n",
      "aau\n",
      "askam\n",
      "specilically\n",
      "ellson\n",
      "favour\n",
      "crht\n",
      "benfro\n",
      "anaesthetist\n",
      "gynaecologist\n",
      "cwb\n",
      "convinved\n",
      "illhealth\n",
      "requirementsof\n",
      "gerasimidis\n",
      "cowley\n",
      "colour\n",
      "carenotes\n",
      "harbour\n",
      "roehampton\n",
      "noemis\n",
      "collative\n",
      "locklow\n",
      "managementthat\n",
      "tidey\n",
      "michale\n",
      "wadduo\n",
      "hcps\n",
      "avallable\n",
      "computerised\n",
      "mhs\n",
      "acuteword\n",
      "sabp\n",
      "professlonal\n",
      "govia\n",
      "analyse\n",
      "observationhatch\n",
      "westyorkshire\n",
      "nikolyan\n",
      "nhse\n",
      "mdt\n",
      "goropriate\n",
      "admisssion\n",
      "toconduct\n",
      "marchessou\n",
      "oinsg\n",
      "tooether\n",
      "keylocks\n",
      "utilised\n",
      "woolwich\n",
      "mtombeni\n",
      "cadat\n",
      "faraday\n",
      "solihull\n",
      "armoury\n",
      "cpft\n",
      "wynnstay\n",
      "learnrestdisclosure\n",
      "neighbourhood\n",
      "watkins\n",
      "ordinators\n",
      "cnwl\n",
      "dowto\n",
      "ssri\n",
      "matthewson\n",
      "ppo\n",
      "dvpn\n",
      "spft\n",
      "shft\n",
      "mhra\n",
      "nps\n",
      "llb\n",
      "beingliquidated\n",
      "eipt\n",
      "cmcus\n",
      "oliv\n",
      "manon\n",
      "fme\n",
      "pactient\n",
      "winslow\n",
      "igor\n",
      "bodycam\n",
      "tavistock\n",
      "diagrammatically\n",
      "accademic\n",
      "martineau\n",
      "thengiv\n",
      "keynes\n",
      "donoghue\n",
      "tolley\n",
      "rcrp\n",
      "gmp\n",
      "anisation\n",
      "hbbt\n",
      "fylde\n",
      "scepticism\n",
      "keywork\n",
      "centre\n",
      "thequota\n",
      "specialise\n",
      "cooney\n",
      "signiflcant\n",
      "hlth\n",
      "scc\n",
      "sacfrom\n",
      "humber\n",
      "ligatured\n",
      "csms\n",
      "westleigh\n",
      "gilva\n",
      "largin\n",
      "demeanour\n",
      "frazer\n",
      "mcnally\n",
      "maudsley\n",
      "kahssay\n",
      "tyneside\n",
      "pyketts\n",
      "delahaye\n",
      "darlington\n",
      "nelft\n",
      "daisu\n",
      "eluned\n",
      "vour\n",
      "javid\n",
      "afzal\n",
      "unusuaf\n",
      "herefordshire\n",
      "dbt\n",
      "awp\n",
      "practises\n",
      "centralised\n",
      "categorise\n",
      "reprecipitated\n",
      "thytly\n",
      "hereals\n",
      "lpft\n",
      "teleohone\n",
      "meaningfuly\n",
      "redbridge\n",
      "wealstun\n",
      "whv\n",
      "warrington\n",
      "inmind\n",
      "hmicfrs\n",
      "iopc\n",
      "fao\n",
      "psirf\n",
      "epjs\n",
      "gloucester\n",
      "psir\n",
      "ittan\n",
      "learnt\n",
      "gloucestershire\n",
      "evelina\n",
      "gdpr\n",
      "quora\n",
      "nwas\n",
      "discrepanciy\n",
      "becklin\n",
      "btp\n",
      "carrpark\n",
      "universtity\n",
      "tokam\n",
      "cypmh\n",
      "fulfil\n",
      "noty\n",
      "gynaecology\n",
      "thrumble\n",
      "usk\n",
      "chlaimpleurbed\n",
      "Total OOV words: 708\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "      <th>WordEmbeddings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[-0.053368486, 0.01803293, 0.02044207, 0.04958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>[process, triaging, prioritise, ambulance, att...</td>\n",
       "      <td>[-0.0029683113, 0.038387537, 0.028525114, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "      <td>[0.016738025, -0.039831422, 0.0031556217, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "      <td>[-0.053647947, 0.010570037, 0.08374962, 0.0514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[-0.017000966, -0.0144550195, 0.013776189, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "      <td>[-0.015921174, 0.03267055, 0.007880878, 0.0818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "      <td>[-0.033860672, 0.02294986, 0.01809456, 0.05957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "      <td>[-0.029299954, 0.021190146, -0.0006726042, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "      <td>[-0.044159707, 0.021678487, 0.021821436, 0.058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...   \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...   \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                LemmatizedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritise, ambulance, att...   \n",
       "Ref: 2024-0298  [question, answer, quora, website, provide, in...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, set, proce...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [bart, london, whilst, clear, evidence, heard,...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commence, ...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seem, evidence, hea...   \n",
       "Ref: 2015-0116  [strip, cell, door, decease, design, engineer,...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                   WordEmbeddings  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [-0.053368486, 0.01803293, 0.02044207, 0.04958...  \n",
       "Ref: 2024-0311  [-0.0029683113, 0.038387537, 0.028525114, 0.05...  \n",
       "Ref: 2024-0298  [0.016738025, -0.039831422, 0.0031556217, 0.05...  \n",
       "Ref: 2024-0297  [-0.053647947, 0.010570037, 0.08374962, 0.0514...  \n",
       "Ref: 2024-0296  [-0.017000966, -0.0144550195, 0.013776189, 0.0...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [-0.015921174, 0.03267055, 0.007880878, 0.0818...  \n",
       "Ref: 2015-0465  [-0.033860672, 0.02294986, 0.01809456, 0.05957...  \n",
       "Ref: 2015-0173  [-0.029299954, 0.021190146, -0.0006726042, 0.0...  \n",
       "Ref: 2015-0116  [-0.044159707, 0.021678487, 0.021821436, 0.058...  \n",
       "Ref: 2015-0072  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[415 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Function to get the average word vector for our lemmatized tokens\n",
    "def embed(tokens, model, oov_words):\n",
    "    valid_tokens = [token for token in tokens if token in model.key_to_index]\n",
    "    oov_tokens = [token for token in tokens if token not in model.key_to_index]\n",
    "    oov_words.update(oov_tokens)\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Initialize the WordEmbeddings column\n",
    "data['WordEmbeddings'] = None\n",
    "\n",
    "# Initialize a set to store OOV words\n",
    "oov_words = set()\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for i, row in data.iterrows():\n",
    "    data.at[i, 'WordEmbeddings'] = embed(row['LemmatizedContent'], model, oov_words)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')\n",
    "\n",
    "# Print the OOV words\n",
    "print(\"Out-of-vocabulary words:\")\n",
    "for word in oov_words:\n",
    "    print(word)\n",
    "\n",
    "# Optionally, you can also print the total count of OOV words\n",
    "print(f'Total OOV words: {len(oov_words)}')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('../Data/tokenised.json', orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanContent</th>\n",
       "      <th>ProcessedContent</th>\n",
       "      <th>LemmatizedContent</th>\n",
       "      <th>WordEmbeddings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0318</th>\n",
       "      <td>Pre-amble Mr Larsen was a 52 year old male wi...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[mr, larsen, year, old, male, history, mental,...</td>\n",
       "      <td>[-0.053368486, 0.01803293, 0.02044207, 0.04958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0311</th>\n",
       "      <td>(1) The process for triaging and prioritising ...</td>\n",
       "      <td>[process, triaging, prioritising, ambulance, a...</td>\n",
       "      <td>[process, triaging, prioritise, ambulance, att...</td>\n",
       "      <td>[-0.0029683113, 0.038387537, 0.028525114, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0298</th>\n",
       "      <td>(1) There are questions and answers on Quora’s...</td>\n",
       "      <td>[questions, answers, quora, website, provide, ...</td>\n",
       "      <td>[question, answer, quora, website, provide, in...</td>\n",
       "      <td>[0.016738025, -0.039831422, 0.0031556217, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0297</th>\n",
       "      <td>(1) The prison service instruction (PSI) 64/20...</td>\n",
       "      <td>[prison, service, instruction, psi, sets, proc...</td>\n",
       "      <td>[prison, service, instruction, psi, set, proce...</td>\n",
       "      <td>[-0.053647947, 0.010570037, 0.08374962, 0.0514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2024-0296</th>\n",
       "      <td>My principal concern is that when a high-risk ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[principal, concern, mental, health, patient, ...</td>\n",
       "      <td>[-0.017000966, -0.0144550195, 0.013776189, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2016-0037</th>\n",
       "      <td>Barts and the London 1. Whilst it was clear to...</td>\n",
       "      <td>[barts, london, whilst, clear, evidence, heard...</td>\n",
       "      <td>[bart, london, whilst, clear, evidence, heard,...</td>\n",
       "      <td>[-0.015921174, 0.03267055, 0.007880878, 0.0818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0465</th>\n",
       "      <td>1. Piotr Kucharz was a Polish gentleman who co...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commenced,...</td>\n",
       "      <td>[piotr, kucharz, polish, gentleman, commence, ...</td>\n",
       "      <td>[-0.033860672, 0.02294986, 0.01809456, 0.05957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0173</th>\n",
       "      <td>Camden and Islington Trust 1. It seemed from t...</td>\n",
       "      <td>[camden, islington, trust, seemed, evidence, h...</td>\n",
       "      <td>[camden, islington, trust, seem, evidence, hea...</td>\n",
       "      <td>[-0.029299954, 0.021190146, -0.0006726042, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0116</th>\n",
       "      <td>NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...</td>\n",
       "      <td>[strips, cell, doors, deceased, design, engine...</td>\n",
       "      <td>[strip, cell, door, decease, design, engineer,...</td>\n",
       "      <td>[-0.044159707, 0.021678487, 0.021821436, 0.058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref: 2015-0072</th>\n",
       "      <td>\"\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CleanContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318   Pre-amble Mr Larsen was a 52 year old male wi...   \n",
       "Ref: 2024-0311  (1) The process for triaging and prioritising ...   \n",
       "Ref: 2024-0298  (1) There are questions and answers on Quora’s...   \n",
       "Ref: 2024-0297  (1) The prison service instruction (PSI) 64/20...   \n",
       "Ref: 2024-0296  My principal concern is that when a high-risk ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  Barts and the London 1. Whilst it was clear to...   \n",
       "Ref: 2015-0465  1. Piotr Kucharz was a Polish gentleman who co...   \n",
       "Ref: 2015-0173  Camden and Islington Trust 1. It seemed from t...   \n",
       "Ref: 2015-0116  NOMS/SODEXO - ANTI-LIGATURE STRIPS ON CELL DOO...   \n",
       "Ref: 2015-0072                                                 \"\"   \n",
       "\n",
       "                                                 ProcessedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritising, ambulance, a...   \n",
       "Ref: 2024-0298  [questions, answers, quora, website, provide, ...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, sets, proc...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [barts, london, whilst, clear, evidence, heard...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commenced,...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seemed, evidence, h...   \n",
       "Ref: 2015-0116  [strips, cell, doors, deceased, design, engine...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                LemmatizedContent  \\\n",
       "ID                                                                  \n",
       "Ref: 2024-0318  [mr, larsen, year, old, male, history, mental,...   \n",
       "Ref: 2024-0311  [process, triaging, prioritise, ambulance, att...   \n",
       "Ref: 2024-0298  [question, answer, quora, website, provide, in...   \n",
       "Ref: 2024-0297  [prison, service, instruction, psi, set, proce...   \n",
       "Ref: 2024-0296  [principal, concern, mental, health, patient, ...   \n",
       "...                                                           ...   \n",
       "Ref: 2016-0037  [bart, london, whilst, clear, evidence, heard,...   \n",
       "Ref: 2015-0465  [piotr, kucharz, polish, gentleman, commence, ...   \n",
       "Ref: 2015-0173  [camden, islington, trust, seem, evidence, hea...   \n",
       "Ref: 2015-0116  [strip, cell, door, decease, design, engineer,...   \n",
       "Ref: 2015-0072                                                 []   \n",
       "\n",
       "                                                   WordEmbeddings  \n",
       "ID                                                                 \n",
       "Ref: 2024-0318  [-0.053368486, 0.01803293, 0.02044207, 0.04958...  \n",
       "Ref: 2024-0311  [-0.0029683113, 0.038387537, 0.028525114, 0.05...  \n",
       "Ref: 2024-0298  [0.016738025, -0.039831422, 0.0031556217, 0.05...  \n",
       "Ref: 2024-0297  [-0.053647947, 0.010570037, 0.08374962, 0.0514...  \n",
       "Ref: 2024-0296  [-0.017000966, -0.0144550195, 0.013776189, 0.0...  \n",
       "...                                                           ...  \n",
       "Ref: 2016-0037  [-0.015921174, 0.03267055, 0.007880878, 0.0818...  \n",
       "Ref: 2015-0465  [-0.033860672, 0.02294986, 0.01809456, 0.05957...  \n",
       "Ref: 2015-0173  [-0.029299954, 0.021190146, -0.0006726042, 0.0...  \n",
       "Ref: 2015-0116  [-0.044159707, 0.021678487, 0.021821436, 0.058...  \n",
       "Ref: 2015-0072  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[415 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded = pd.read_json('../Data/tokenised.json', orient = 'split')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align NLTK tokens with BERT tokens\n",
    "\n",
    "def get_embeddings_for_token(tokens):\n",
    "    token_embeddings = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Tokenize the word\n",
    "        tokenized_word = tokenizer.tokenize(token)\n",
    "        \n",
    "        # Convert tokenized word to input IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "        \n",
    "        # Convert input IDs to tensor\n",
    "        input_ids_tensor = torch.tensor([input_ids])\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids_tensor)\n",
    "        \n",
    "        # Get the embeddings for the subwords\n",
    "        subword_embeddings = outputs.last_hidden_state.squeeze(0).numpy()\n",
    "        \n",
    "        # Average the subword embeddings to get a single embedding per token\n",
    "        token_embedding = np.mean(subword_embeddings, axis=0)\n",
    "        \n",
    "        token_embeddings.append(token_embedding)\n",
    "    \n",
    "    return np.array(token_embeddings)\n",
    "\n",
    "data['WordEmbeddings'] = data['Tokens'].apply(get_embeddings_for_token)\n",
    "\n",
    "print(\"Word Embeddings:\")\n",
    "print(data['WordEmbeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import python_utils\n",
    "\n",
    "# First, we need to make sure that no report exceeds the max number of tokens (8000) specified by OpenAI. This will prevent server-side errors.\n",
    "\n",
    "def num_tokens_from_text(text: str, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Returns the number of OpenAI tokens.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "# Function to un-tokenize text\n",
    "def untokenize(tokens):\n",
    "    \"\"\"\n",
    "    Reverses the tokenization process.\n",
    "    \"\"\"\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply `untokenize`` function\n",
    "data['LemmatizedContent'] = data['LemmatizedContent'].apply(untokenize)\n",
    "\n",
    "# Calculate the token count\n",
    "data['TokenCount_LemmatizedContent'] = data['LemmatizedContent'].apply(num_tokens_from_text)\n",
    "\n",
    "# Count the number of reports that exceed the maximum number of tokens\n",
    "max_tokens = 8000\n",
    "exceeding_reports_count = (data['TokenCount_LemmatizedContent'] > max_tokens).sum()\n",
    "\n",
    "print(f\"Number of reports exceeding the maximum number of tokens: {exceeding_reports_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up OpenAI API\n",
    "load_dotenv('api.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Create function that provides text embeddings\n",
    "def get_embedding(text_to_embbed, model_ID):\n",
    "    text = text_to_embbed.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text_to_embbed], model=model_ID).data[0].embedding\n",
    "\n",
    "# Define empty list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Reset index to avoid bug\n",
    "data = data.reset_index(drop=False)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over each element of \"LemmatizedContent\" field and get embeddings\n",
    "for idx, text in enumerate(data['LemmatizedContent']):\n",
    "    print('Processing row {i} of {n}'.format(i=idx, n=len(data)))\n",
    "    try:\n",
    "        embedding = get_embedding(text, \"text-embedding-3-large\")\n",
    "        embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        embeddings.append(None)\n",
    "        print(f'Error on row {idx}: {e}')\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate & print time taken\n",
    "total_time = end_time - start_time\n",
    "minutes = int(total_time // 60)\n",
    "seconds = total_time % 60\n",
    "\n",
    "print(f'Time taken: {minutes} minutes and {seconds:.2f} seconds')\n",
    "\n",
    "# Add embeddings to the dataframe\n",
    "data['text-embedding-3-large'] = embeddings\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the above broadly worked, but the console is printing lots of errors. We can investigate these individual reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print row 174 in \"data\"\n",
    "print(data.loc[174])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these errors exist because our the could not remove the intro text in `preprocess.ipynb`, and provided an empty string. This is fine for now, as the number of errors is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenise the data\n",
    "data['LemmatizedContent'] = data['LemmatizedContent'].apply(word_tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['LemmatizedContent'][0]), len(data['text-embedding-3-large'][0]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and process data\n",
    "\n",
    "Before topic modelling, we need to: (1) tokenize the data; (2) remove punctuation, special characters and numbers; (3) remove stop words; (4) lemmatize tokens to their dictionary base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenise the report content\n",
    "data['TokenisedContent'] = data['CleanContent'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation, special characters and numbers\n",
    "data['TokenisedContent'] = data['TokenisedContent'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['TokenisedContent'] = data['TokenisedContent'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row of \"data\" with ID of \"Ref: 2015-0072\" due to erronous output\n",
    "# This is a temporary fix. Some prompt engineering is needed in the OpenAI API call to prevent erronous outputs\n",
    "data = data.drop('Ref: 2015-0072')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
